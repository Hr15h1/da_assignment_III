{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab30a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_df = pd.read_csv('mnist_train.csv')\n",
    "mnist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49654a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_df = pd.read_csv('mnist_test.csv')\n",
    "mnist_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eafc88",
   "metadata": {},
   "source": [
    "Train a base model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ccc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_X = mnist_df.drop(columns=[\"label\"]).values\n",
    "mnist_y = mnist_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c832aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_X = mnist_test_df.drop(columns=[\"label\"]).values\n",
    "mnist_test_y = mnist_test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dec5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx].reshape(28, 28).astype(\"float32\") / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        img = img.repeat(3, 1, 1)\n",
    "\n",
    "        img = F.interpolate(img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "train_dataset = MNISTDataset(mnist_X, mnist_y)\n",
    "test_dataset = MNISTDataset(mnist_test_X, mnist_test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660df9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "base_model.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55613ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8948f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ecb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "base_loss_list = []\n",
    "base_accuracy_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    base_model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = base_model(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    base_loss_list.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    base_model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = base_model(X_batch)\n",
    "            test_loss = loss_fn(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    base_accuracy_list.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(base_model.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4469f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    base_model.load_state_dict(best_model_weights)\n",
    "    base_model.eval()\n",
    "    base_model.to(device)\n",
    "\n",
    "    y_pred = []\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = base_model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(mnist_test_y, y_pred)\n",
    "conf_matrix = confusion_matrix(mnist_test_y, y_pred)\n",
    "class_report = classification_report(mnist_test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of base model = {accuracy}\")\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Base Model')\n",
    "plt.show()\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb1e90",
   "metadata": {},
   "source": [
    "Dividing training data into 10% labelled and 90% unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8718fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "\n",
    "for _, subset_idx in strat_split.split(mnist_X, mnist_y):\n",
    "    labeled_X = mnist_X[subset_idx]\n",
    "    y = mnist_y[subset_idx]\n",
    "    unlabeled_X = mnist_X[_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e797538",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for test, train in strat_split1.split(labeled_X, y):\n",
    "    X_train = labeled_X[train]\n",
    "    y_train = y[train]\n",
    "    X_val = labeled_X[test]\n",
    "    y_val = y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bb87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_dataset = MNISTDataset(labeled_X, y)\n",
    "train_loader = DataLoader(labeled_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = MNISTDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "unlabeled_dataset = MNISTDataset(unlabeled_X, np.zeros(len(unlabeled_X)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87be90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model1.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "\n",
    "model1.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "loss_list_10 = []\n",
    "accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model1.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model1(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model1.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model1(X_batch)\n",
    "            test_loss = loss_fn(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model1.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77581920",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_images_07 = []\n",
    "pseudo_labels_07 = []\n",
    "pseudo_images_08 = []\n",
    "pseudo_labels_08 = []\n",
    "pseudo_images_09 = []\n",
    "pseudo_labels_09 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model1.load_state_dict(best_model_weights)\n",
    "    model1.eval()\n",
    "    model1.to(device)\n",
    "\n",
    "    for images, _ in unlabeled_X:\n",
    "        images = images.to(device)\n",
    "        outputs = model1(images)\n",
    "        preds = F.softmax(outputs, dim=1)\n",
    "\n",
    "        conf, preds = torch.max(preds, dim=1)\n",
    "        mask_07 = conf >= 0.7\n",
    "        mask_08 = conf >= 0.8\n",
    "        mask_09 = conf >= 0.9\n",
    "\n",
    "        pseudo_images_07.append(images[mask_07].cpu())\n",
    "        pseudo_labels_07.append(preds[mask_07].cpu())\n",
    "        pseudo_images_08.append(images[mask_08].cpu())\n",
    "        pseudo_labels_08.append(preds[mask_08].cpu())\n",
    "        pseudo_images_09.append(images[mask_09].cpu())\n",
    "        pseudo_labels_09.append(preds[mask_09].cpu())\n",
    "\n",
    "pseudo_images_07 = torch.cat(pseudo_images_07, dim=0)\n",
    "pseudo_labels_07 = torch.cat(pseudo_labels_07, dim=0)\n",
    "pseudo_images_08 = torch.cat(pseudo_images_08, dim=0)\n",
    "pseudo_labels_08 = torch.cat(pseudo_labels_08, dim=0)\n",
    "pseudo_images_09 = torch.cat(pseudo_images_09, dim=0)\n",
    "pseudo_labels_09 = torch.cat(pseudo_labels_09, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9da5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx].reshape(28, 28).astype(\"float32\") / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        img = img.repeat(3, 1, 1)\n",
    "\n",
    "        img = F.interpolate(img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "    \n",
    "combined_dataset_07 = ConcatDataset([PseudoDataset(pseudo_images_07, pseudo_labels_07), labeled_train_dataset])\n",
    "combined_loader_07 = DataLoader(combined_dataset_07, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_08 = ConcatDataset([PseudoDataset(pseudo_images_08, pseudo_labels_08), labeled_train_dataset])\n",
    "combined_loader_08 = DataLoader(combined_dataset_08, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_09 = ConcatDataset([PseudoDataset(pseudo_images_09, pseudo_labels_09), labeled_train_dataset])\n",
    "combined_loader_09 = DataLoader(combined_dataset_09, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_07 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model1_07.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model1_07.to(device)\n",
    "loss_fn_07 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_07 = torch.optim.Adam(model1_07.parameters(), lr=0.001)\n",
    "\n",
    "model1_08 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model1_08.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model1_08.to(device)\n",
    "loss_fn_08 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_08 = torch.optim.Adam(model1_08.parameters(), lr=0.001)\n",
    "\n",
    "model1_09 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model1_09.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model1_09.to(device)\n",
    "loss_fn_09 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_09 = torch.optim.Adam(model1_09.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c79ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model1_07.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_07:\n",
    "        optimizer_07.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model1_07(X_batch)\n",
    "        loss = loss_fn_07(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_07.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model1_07.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model1_07(X_batch)\n",
    "            test_loss = loss_fn_07(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model1_07.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model1_07.load_state_dict(best_model_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702094c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model1_08.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_08:\n",
    "        optimizer_08.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model1_08(X_batch)\n",
    "        loss = loss_fn_08(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_08.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model1_08.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model1_08(X_batch)\n",
    "            test_loss = loss_fn_08(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model1_08.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model1_08.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb52636",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model1_09.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_09:\n",
    "        optimizer_09.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model1_09(X_batch)\n",
    "        loss = loss_fn_09(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_09.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model1_09.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model1_09(X_batch)\n",
    "            test_loss = loss_fn_09(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model1_09.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model1_09.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909dceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model1_07.eval()\n",
    "    model1_07.to(device)\n",
    "    model1_08.eval()\n",
    "    model1_08.to(device)\n",
    "    model1_09.eval()\n",
    "    model1_09.to(device)\n",
    "\n",
    "    y_pred = []\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs_07 = model1_07(X_batch)\n",
    "        outputs_08 = model1_08(X_batch)\n",
    "        outputs_09 = model1_09(X_batch)\n",
    "\n",
    "        preds_07 = torch.argmax(outputs_07, dim=1)\n",
    "        preds_08 = torch.argmax(outputs_08, dim=1)\n",
    "        preds_09 = torch.argmax(outputs_09, dim=1)\n",
    "\n",
    "\n",
    "accuracy_07 = accuracy_score(mnist_test_y, preds_07)\n",
    "conf_matrix_07 = confusion_matrix(mnist_test_y, preds_07)\n",
    "class_report_07 = classification_report(mnist_test_y, preds_07)\n",
    "\n",
    "accuracy_08 = accuracy_score(mnist_test_y, preds_08)\n",
    "conf_matrix_08 = confusion_matrix(mnist_test_y, preds_08)\n",
    "class_report_08 = classification_report(mnist_test_y, preds_08)\n",
    "\n",
    "accuracy_09 = accuracy_score(mnist_test_y, preds_09)\n",
    "conf_matrix_09 = confusion_matrix(mnist_test_y, preds_09)\n",
    "class_report_09 = classification_report(mnist_test_y, preds_09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of model trained with 10% labeled data and pseudo-labeling:\")\n",
    "print(f\"0.7 Confidence Threshold: {accuracy_07}\")\n",
    "print(f\"0.8 Confidence Threshold: {accuracy_08}\")\n",
    "print(f\"0.9 Confidence Threshold: {accuracy_09}\")\n",
    "\n",
    "plt.figure(figsize=(30, 8))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    if i == 0:\n",
    "        sns.heatmap(conf_matrix_07, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.7 Threshold)')\n",
    "    elif i == 1:\n",
    "        sns.heatmap(conf_matrix_08, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.8 Threshold)')\n",
    "    else:\n",
    "        sns.heatmap(conf_matrix_09, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.9 Threshold)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report (0.7 Threshold):\")\n",
    "print(class_report_07)\n",
    "print(\"\\nClassification Report (0.8 Threshold):\")\n",
    "print(class_report_08)\n",
    "print(\"\\nClassification Report (0.9 Threshold):\")\n",
    "print(class_report_09)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8169a",
   "metadata": {},
   "source": [
    "Training a model on 20% labeled and 80% unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160882fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for _, subset_idx in strat_split.split(mnist_X, mnist_y):\n",
    "    labeled_X = mnist_X[subset_idx]\n",
    "    y = mnist_y[subset_idx]\n",
    "    unlabeled_X = mnist_X[_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b16fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for test, train in strat_split1.split(labeled_X, y):\n",
    "    X_train = labeled_X[train]\n",
    "    y_train = y[train]\n",
    "    X_val = labeled_X[test]\n",
    "    y_val = y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_dataset = MNISTDataset(labeled_X, y)\n",
    "train_loader = DataLoader(labeled_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = MNISTDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "unlabeled_dataset = MNISTDataset(unlabeled_X, np.zeros(len(unlabeled_X)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model2.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "\n",
    "model2.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ac5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "loss_list_10 = []\n",
    "accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model2.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model2(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model2.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model2(X_batch)\n",
    "            test_loss = loss_fn(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model2.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc454bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_images_07 = []\n",
    "pseudo_labels_07 = []\n",
    "pseudo_images_08 = []\n",
    "pseudo_labels_08 = []\n",
    "pseudo_images_09 = []\n",
    "pseudo_labels_09 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model2.load_state_dict(best_model_weights)\n",
    "    model2.eval()\n",
    "    model2.to(device)\n",
    "\n",
    "    for images, _ in unlabeled_X:\n",
    "        images = images.to(device)\n",
    "        outputs = model2(images)\n",
    "        preds = F.softmax(outputs, dim=1)\n",
    "\n",
    "        conf, preds = torch.max(preds, dim=1)\n",
    "        mask_07 = conf >= 0.7\n",
    "        mask_08 = conf >= 0.8\n",
    "        mask_09 = conf >= 0.9\n",
    "\n",
    "        pseudo_images_07.append(images[mask_07].cpu())\n",
    "        pseudo_labels_07.append(preds[mask_07].cpu())\n",
    "        pseudo_images_08.append(images[mask_08].cpu())\n",
    "        pseudo_labels_08.append(preds[mask_08].cpu())\n",
    "        pseudo_images_09.append(images[mask_09].cpu())\n",
    "        pseudo_labels_09.append(preds[mask_09].cpu())\n",
    "\n",
    "pseudo_images_07 = torch.cat(pseudo_images_07, dim=0)\n",
    "pseudo_labels_07 = torch.cat(pseudo_labels_07, dim=0)\n",
    "pseudo_images_08 = torch.cat(pseudo_images_08, dim=0)\n",
    "pseudo_labels_08 = torch.cat(pseudo_labels_08, dim=0)\n",
    "pseudo_images_09 = torch.cat(pseudo_images_09, dim=0)\n",
    "pseudo_labels_09 = torch.cat(pseudo_labels_09, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx].reshape(28, 28).astype(\"float32\") / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        img = img.repeat(3, 1, 1)\n",
    "\n",
    "        img = F.interpolate(img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "    \n",
    "combined_dataset_07 = ConcatDataset([PseudoDataset(pseudo_images_07, pseudo_labels_07), labeled_train_dataset])\n",
    "combined_loader_07 = DataLoader(combined_dataset_07, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_08 = ConcatDataset([PseudoDataset(pseudo_images_08, pseudo_labels_08), labeled_train_dataset])\n",
    "combined_loader_08 = DataLoader(combined_dataset_08, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_09 = ConcatDataset([PseudoDataset(pseudo_images_09, pseudo_labels_09), labeled_train_dataset])\n",
    "combined_loader_09 = DataLoader(combined_dataset_09, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c654cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_07 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model2_07.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model2_07.to(device)\n",
    "loss_fn_07 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_07 = torch.optim.Adam(model2_07.parameters(), lr=0.001)\n",
    "\n",
    "model2_08 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model2_08.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model2_08.to(device)\n",
    "loss_fn_08 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_08 = torch.optim.Adam(model2_08.parameters(), lr=0.001)\n",
    "\n",
    "model2_09 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model2_09.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model2_09.to(device)\n",
    "loss_fn_09 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_09 = torch.optim.Adam(model2_09.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a89f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model2_07.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_07:\n",
    "        optimizer_07.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model2_07(X_batch)\n",
    "        loss = loss_fn_07(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_07.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model2_07.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model2_07(X_batch)\n",
    "            test_loss = loss_fn_07(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model2_07.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model2_07.load_state_dict(best_model_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model2_08.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_08:\n",
    "        optimizer_08.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model2_08(X_batch)\n",
    "        loss = loss_fn_08(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_08.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model2_08.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model2_08(X_batch)\n",
    "            test_loss = loss_fn_08(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model2_08.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model2_08.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model2_09.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_09:\n",
    "        optimizer_09.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model2_09(X_batch)\n",
    "        loss = loss_fn_09(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_09.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model2_09.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model2_09(X_batch)\n",
    "            test_loss = loss_fn_09(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model2_09.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model2_09.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc9e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model2_07.eval()\n",
    "    model2_07.to(device)\n",
    "    model2_08.eval()\n",
    "    model2_08.to(device)\n",
    "    model2_09.eval()\n",
    "    model2_09.to(device)\n",
    "\n",
    "    y_pred = []\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs_07 = model2_07(X_batch)\n",
    "        outputs_08 = model2_08(X_batch)\n",
    "        outputs_09 = model2_09(X_batch)\n",
    "\n",
    "        preds_07 = torch.argmax(outputs_07, dim=1)\n",
    "        preds_08 = torch.argmax(outputs_08, dim=1)\n",
    "        preds_09 = torch.argmax(outputs_09, dim=1)\n",
    "\n",
    "\n",
    "accuracy_07 = accuracy_score(mnist_test_y, preds_07)\n",
    "conf_matrix_07 = confusion_matrix(mnist_test_y, preds_07)\n",
    "class_report_07 = classification_report(mnist_test_y, preds_07)\n",
    "\n",
    "accuracy_08 = accuracy_score(mnist_test_y, preds_08)\n",
    "conf_matrix_08 = confusion_matrix(mnist_test_y, preds_08)\n",
    "class_report_08 = classification_report(mnist_test_y, preds_08)\n",
    "\n",
    "accuracy_09 = accuracy_score(mnist_test_y, preds_09)\n",
    "conf_matrix_09 = confusion_matrix(mnist_test_y, preds_09)\n",
    "class_report_09 = classification_report(mnist_test_y, preds_09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cabaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of model trained with 20% labeled data and pseudo-labeling:\")\n",
    "print(f\"0.7 Confidence Threshold: {accuracy_07}\")\n",
    "print(f\"0.8 Confidence Threshold: {accuracy_08}\")\n",
    "print(f\"0.9 Confidence Threshold: {accuracy_09}\")\n",
    "\n",
    "plt.figure(figsize=(30, 8))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    if i == 0:\n",
    "        sns.heatmap(conf_matrix_07, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.7 Threshold)')\n",
    "    elif i == 1:\n",
    "        sns.heatmap(conf_matrix_08, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.8 Threshold)')\n",
    "    else:\n",
    "        sns.heatmap(conf_matrix_09, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.9 Threshold)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report (0.7 Threshold):\")\n",
    "print(class_report_07)\n",
    "print(\"\\nClassification Report (0.8 Threshold):\")\n",
    "print(class_report_08)\n",
    "print(\"\\nClassification Report (0.9 Threshold):\")\n",
    "print(class_report_09)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4813d27",
   "metadata": {},
   "source": [
    "Training a model on 30% labeled and 70% unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for _, subset_idx in strat_split.split(mnist_X, mnist_y):\n",
    "    labeled_X = mnist_X[subset_idx]\n",
    "    y = mnist_y[subset_idx]\n",
    "    unlabeled_X = mnist_X[_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc66cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for test, train in strat_split1.split(labeled_X, y):\n",
    "    X_train = labeled_X[train]\n",
    "    y_train = y[train]\n",
    "    X_val = labeled_X[test]\n",
    "    y_val = y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_dataset = MNISTDataset(labeled_X, y)\n",
    "train_loader = DataLoader(labeled_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = MNISTDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "unlabeled_dataset = MNISTDataset(unlabeled_X, np.zeros(len(unlabeled_X)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cdf84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model3.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "\n",
    "model3.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "loss_list_10 = []\n",
    "accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model3.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model3(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model3.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model3(X_batch)\n",
    "            test_loss = loss_fn(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model3.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_images_07 = []\n",
    "pseudo_labels_07 = []\n",
    "pseudo_images_08 = []\n",
    "pseudo_labels_08 = []\n",
    "pseudo_images_09 = []\n",
    "pseudo_labels_09 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model3.load_state_dict(best_model_weights)\n",
    "    model3.eval()\n",
    "    model3.to(device)\n",
    "\n",
    "    for images, _ in unlabeled_X:\n",
    "        images = images.to(device)\n",
    "        outputs = model3(images)\n",
    "        preds = F.softmax(outputs, dim=1)\n",
    "\n",
    "        conf, preds = torch.max(preds, dim=1)\n",
    "        mask_07 = conf >= 0.7\n",
    "        mask_08 = conf >= 0.8\n",
    "        mask_09 = conf >= 0.9\n",
    "\n",
    "        pseudo_images_07.append(images[mask_07].cpu())\n",
    "        pseudo_labels_07.append(preds[mask_07].cpu())\n",
    "        pseudo_images_08.append(images[mask_08].cpu())\n",
    "        pseudo_labels_08.append(preds[mask_08].cpu())\n",
    "        pseudo_images_09.append(images[mask_09].cpu())\n",
    "        pseudo_labels_09.append(preds[mask_09].cpu())\n",
    "\n",
    "pseudo_images_07 = torch.cat(pseudo_images_07, dim=0)\n",
    "pseudo_labels_07 = torch.cat(pseudo_labels_07, dim=0)\n",
    "pseudo_images_08 = torch.cat(pseudo_images_08, dim=0)\n",
    "pseudo_labels_08 = torch.cat(pseudo_labels_08, dim=0)\n",
    "pseudo_images_09 = torch.cat(pseudo_images_09, dim=0)\n",
    "pseudo_labels_09 = torch.cat(pseudo_labels_09, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx].reshape(28, 28).astype(\"float32\") / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        img = img.repeat(3, 1, 1)\n",
    "\n",
    "        img = F.interpolate(img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "    \n",
    "combined_dataset_07 = ConcatDataset([PseudoDataset(pseudo_images_07, pseudo_labels_07), labeled_train_dataset])\n",
    "combined_loader_07 = DataLoader(combined_dataset_07, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_08 = ConcatDataset([PseudoDataset(pseudo_images_08, pseudo_labels_08), labeled_train_dataset])\n",
    "combined_loader_08 = DataLoader(combined_dataset_08, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_09 = ConcatDataset([PseudoDataset(pseudo_images_09, pseudo_labels_09), labeled_train_dataset])\n",
    "combined_loader_09 = DataLoader(combined_dataset_09, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf13f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_07 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model3_07.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model3_07.to(device)\n",
    "loss_fn_07 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_07 = torch.optim.Adam(model3_07.parameters(), lr=0.001)\n",
    "\n",
    "model3_08 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model3_08.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model3_08.to(device)\n",
    "loss_fn_08 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_08 = torch.optim.Adam(model3_08.parameters(), lr=0.001)\n",
    "\n",
    "model3_09 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model3_09.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model3_09.to(device)\n",
    "loss_fn_09 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_09 = torch.optim.Adam(model3_09.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9afc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model3_07.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_07:\n",
    "        optimizer_07.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model3_07(X_batch)\n",
    "        loss = loss_fn_07(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_07.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model3_07.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model3_07(X_batch)\n",
    "            test_loss = loss_fn_07(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model3_07.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model3_07.load_state_dict(best_model_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7aa18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model3_08.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_08:\n",
    "        optimizer_08.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model3_08(X_batch)\n",
    "        loss = loss_fn_08(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_08.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model3_08.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model3_08(X_batch)\n",
    "            test_loss = loss_fn_08(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model3_08.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model3_08.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa62410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model3_09.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_09:\n",
    "        optimizer_09.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model3_09(X_batch)\n",
    "        loss = loss_fn_09(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_09.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model3_09.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model3_09(X_batch)\n",
    "            test_loss = loss_fn_09(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model3_09.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model3_09.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15356c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model3_07.eval()\n",
    "    model3_07.to(device)\n",
    "    model3_08.eval()\n",
    "    model3_08.to(device)\n",
    "    model3_09.eval()\n",
    "    model3_09.to(device)\n",
    "\n",
    "    y_pred = []\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs_07 = model3_07(X_batch)\n",
    "        outputs_08 = model3_08(X_batch)\n",
    "        outputs_09 = model3_09(X_batch)\n",
    "\n",
    "        preds_07 = torch.argmax(outputs_07, dim=1)\n",
    "        preds_08 = torch.argmax(outputs_08, dim=1)\n",
    "        preds_09 = torch.argmax(outputs_09, dim=1)\n",
    "\n",
    "\n",
    "accuracy_07 = accuracy_score(mnist_test_y, preds_07)\n",
    "conf_matrix_07 = confusion_matrix(mnist_test_y, preds_07)\n",
    "class_report_07 = classification_report(mnist_test_y, preds_07)\n",
    "\n",
    "accuracy_08 = accuracy_score(mnist_test_y, preds_08)\n",
    "conf_matrix_08 = confusion_matrix(mnist_test_y, preds_08)\n",
    "class_report_08 = classification_report(mnist_test_y, preds_08)\n",
    "\n",
    "accuracy_09 = accuracy_score(mnist_test_y, preds_09)\n",
    "conf_matrix_09 = confusion_matrix(mnist_test_y, preds_09)\n",
    "class_report_09 = classification_report(mnist_test_y, preds_09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2817e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of model trained with 30% labeled data and pseudo-labeling:\")\n",
    "print(f\"0.7 Confidence Threshold: {accuracy_07}\")\n",
    "print(f\"0.8 Confidence Threshold: {accuracy_08}\")\n",
    "print(f\"0.9 Confidence Threshold: {accuracy_09}\")\n",
    "\n",
    "plt.figure(figsize=(30, 8))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    if i == 0:\n",
    "        sns.heatmap(conf_matrix_07, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.7 Threshold)')\n",
    "    elif i == 1:\n",
    "        sns.heatmap(conf_matrix_08, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.8 Threshold)')\n",
    "    else:\n",
    "        sns.heatmap(conf_matrix_09, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.9 Threshold)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report (0.7 Threshold):\")\n",
    "print(class_report_07)\n",
    "print(\"\\nClassification Report (0.8 Threshold):\")\n",
    "print(class_report_08)\n",
    "print(\"\\nClassification Report (0.9 Threshold):\")\n",
    "print(class_report_09)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
