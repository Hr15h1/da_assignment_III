{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab30a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc0cfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0          5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1          0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2          4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3          1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4          9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "59995      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59996      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59997      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59998      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59999      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "       28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0          0      0      0      0      0      0      0      0  \n",
       "1          0      0      0      0      0      0      0      0  \n",
       "2          0      0      0      0      0      0      0      0  \n",
       "3          0      0      0      0      0      0      0      0  \n",
       "4          0      0      0      0      0      0      0      0  \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "59995      0      0      0      0      0      0      0      0  \n",
       "59996      0      0      0      0      0      0      0      0  \n",
       "59997      0      0      0      0      0      0      0      0  \n",
       "59998      0      0      0      0      0      0      0      0  \n",
       "59999      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[60000 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_df = pd.read_csv('mnist_train.csv')\n",
    "mnist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49654a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0         7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1         2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2         1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3         0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4         4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "9995      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "9996      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "9997      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "9998      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "9999      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "      28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0         0      0      0      0      0      0      0      0  \n",
       "1         0      0      0      0      0      0      0      0  \n",
       "2         0      0      0      0      0      0      0      0  \n",
       "3         0      0      0      0      0      0      0      0  \n",
       "4         0      0      0      0      0      0      0      0  \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "9995      0      0      0      0      0      0      0      0  \n",
       "9996      0      0      0      0      0      0      0      0  \n",
       "9997      0      0      0      0      0      0      0      0  \n",
       "9998      0      0      0      0      0      0      0      0  \n",
       "9999      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[10000 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test_df = pd.read_csv('mnist_test.csv')\n",
    "mnist_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eafc88",
   "metadata": {},
   "source": [
    "Train a base model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4ccc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_X = mnist_df.drop(columns=[\"label\"]).values\n",
    "mnist_y = mnist_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c832aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_X = mnist_test_df.drop(columns=[\"label\"]).values\n",
    "mnist_test_y = mnist_test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99dec5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5678/2126796601.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n",
      "  self.y = torch.as_tensor(y, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.as_tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx].reshape(28, 28) / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        img = img.repeat(3, 1, 1)\n",
    "\n",
    "        img = F.interpolate(img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "train_dataset = MNISTDataset(mnist_X, mnist_y)\n",
    "test_dataset = MNISTDataset(mnist_test_X, mnist_test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "660df9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "base_model.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55613ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db8948f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a0ecb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5678/2126796601.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img).unsqueeze(0)\n",
      "/tmp/ipykernel_5678/2126796601.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.y[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.0690, Val Loss: 0.0318, Val Accuracy: 0.9895\n",
      "Epoch 2/1000\n",
      "Epoch 2/1000, Train Loss: 0.0298, Val Loss: 0.0525, Val Accuracy: 0.9855\n",
      "Epoch 3/1000\n",
      "Epoch 3/1000, Train Loss: 0.0264, Val Loss: 0.0573, Val Accuracy: 0.9850\n",
      "Epoch 4/1000\n",
      "Epoch 4/1000, Train Loss: 0.0224, Val Loss: 0.0234, Val Accuracy: 0.9928\n",
      "Epoch 5/1000\n",
      "Epoch 5/1000, Train Loss: 0.0192, Val Loss: 0.0318, Val Accuracy: 0.9907\n",
      "Epoch 6/1000\n",
      "Epoch 6/1000, Train Loss: 0.0177, Val Loss: 0.0834, Val Accuracy: 0.9780\n",
      "Epoch 7/1000\n",
      "Epoch 7/1000, Train Loss: 0.0203, Val Loss: 0.0173, Val Accuracy: 0.9953\n",
      "Epoch 8/1000\n",
      "Epoch 8/1000, Train Loss: 0.0149, Val Loss: 0.0367, Val Accuracy: 0.9895\n",
      "Epoch 9/1000\n",
      "Epoch 9/1000, Train Loss: 0.0136, Val Loss: 0.0211, Val Accuracy: 0.9947\n",
      "Epoch 10/1000\n",
      "Epoch 10/1000, Train Loss: 0.0142, Val Loss: 0.0384, Val Accuracy: 0.9899\n",
      "Epoch 11/1000\n",
      "Epoch 11/1000, Train Loss: 0.0132, Val Loss: 0.0300, Val Accuracy: 0.9924\n",
      "Epoch 12/1000\n",
      "Epoch 12/1000, Train Loss: 0.0094, Val Loss: 0.0304, Val Accuracy: 0.9941\n",
      "Epoch 13/1000\n",
      "Epoch 13/1000, Train Loss: 0.0119, Val Loss: 0.0210, Val Accuracy: 0.9954\n",
      "Epoch 14/1000\n",
      "Early stopping at epoch 14\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "base_loss_list = []\n",
    "base_accuracy_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    base_model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = base_model(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    base_loss_list.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    base_model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = base_model(X_batch)\n",
    "            test_loss = loss_fn(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    base_accuracy_list.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(base_model.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4469f5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5678/2126796601.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img).unsqueeze(0)\n",
      "/tmp/ipykernel_5678/2126796601.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.y[idx], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    base_model.load_state_dict(best_model_weights)\n",
    "    base_model.eval()\n",
    "    base_model.to(device)\n",
    "\n",
    "    y_pred = []\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = base_model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(mnist_test_y, y_pred)\n",
    "conf_matrix = confusion_matrix(mnist_test_y, y_pred)\n",
    "class_report = classification_report(mnist_test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7423ca26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of base model = 0.9953\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxkAAAJwCAYAAADlb6zZAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAi+xJREFUeJzs3Xl8TNf/x/H3JLIQsgjZtHZiX2qNvaQUra2oVhVFUVpLq0qpWiqluqlauuFrqepCW22V0lIVsbT2vZRakhAklkhI5veHn2mmQRMzmZvMvJ7fx338fjn3zJ3PZ86V5sznnntNZrPZLAAAAACwEzejAwAAAADgXJhkAAAAALArJhkAAAAA7IpJBgAAAAC7YpIBAAAAwK6YZAAAAACwKyYZAAAAAOyKSQYAAAAAu2KSAQAAAMCumGQAuKVDhw6pZcuW8vPzk8lk0vLly+16/L/++ksmk0nz5s2z63HzsmbNmqlZs2Z2O96lS5fUt29fhYSEyGQyaejQoXY7Nmxny3iXLFlSvXr1sms8AGBPTDKAXOzPP/9U//79Vbp0aXl7e8vX11cNGzbUu+++q+Tk5Bx97549e2rXrl167bXXtGDBAtWuXTtH38+RevXqJZPJJF9f31t+jocOHZLJZJLJZNK0adOyffxTp07p1Vdf1fbt2+0Q7d2bPHmy5s2bp4EDB2rBggXq0aNHjr5fyZIlLZ+byWSSt7e3ypUrpxEjRujcuXM5+t536+Zk12QyadKkSbfs0717d5lMJhUsWNDB0QFA3pXP6AAA3Np3332nLl26yMvLS08++aSqVKmi1NRUbdiwQSNGjNCePXv0wQcf5Mh7JycnKzo6Wi+//LIGDx6cI+9RokQJJScny8PDI0eO/1/y5cunK1eu6Ntvv1XXrl2t9i1atEje3t66evXqXR371KlTGj9+vEqWLKkaNWpk+XWrVq26q/e7nbVr16p+/foaN26cXY97JzVq1NDzzz8vSbp69aq2bdumd955R+vWrdPmzZsdFkd2eXt769NPP9WYMWOs2i9fvqyvv/5a3t7eBkUGAHkTkwwgFzp69Ki6deumEiVKaO3atQoNDbXsGzRokA4fPqzvvvsux97/zJkzkiR/f/8ce4+b33QbxcvLSw0bNtSnn36aaZKxePFitW3bVl9++aVDYrly5YoKFCggT09Pux43Pj5elSpVstvxrl+/rvT09DvGWaxYMT3xxBOWn/v27auCBQtq2rRpOnTokMqVK2e3eOypTZs2+uqrr7Rjxw5Vr17d0v71118rNTVVDz74oNauXWtghACQt3C5FJALTZ06VZcuXdLHH39sNcG4qWzZshoyZIjl5+vXr2vixIkqU6aMvLy8VLJkSY0ePVopKSlWrytZsqQeeughbdiwQXXr1pW3t7dKly6t//3vf5Y+r776qkqUKCFJGjFihEwmk0qWLCnpxmVGN///jF599VWZTCarttWrV6tRo0by9/dXwYIFFR4ertGjR1v2325Nxtq1a9W4cWP5+PjI399f7du31759+275focPH1avXr3k7+8vPz8/9e7dW1euXLn9B/svjz/+uH744QdduHDB0rZlyxYdOnRIjz/+eKb+586d0wsvvKCqVauqYMGC8vX1VevWrbVjxw5Ln19++UV16tSRJPXu3dtyKc7NPJs1a6YqVapo27ZtatKkiQoUKGD5XP59jX7Pnj3l7e2dKf9WrVopICBAp06dumVev/zyi0wmk44eParvvvvOEsNff/0l6cbko0+fPgoODpa3t7eqV6+u+fPnWx3j5vhMmzZN77zzjuXc2rt3b5Y+24xCQkIk3age3bRz50716tXLcilgSEiInnrqKSUkJFi99uLFixo6dKhKliwpLy8vBQUF6YEHHtDvv/9u1S8mJkYPPvig/Pz8VKBAATVt2lS//fZblmOMiIhQqVKltHjxYqv2RYsW6cEHH1ThwoVv+bqZM2eqcuXK8vLyUlhYmAYNGmR1Pt30wQcfqEyZMsqfP7/q1q2rX3/99ZbHS0lJ0bhx41S2bFl5eXnp3nvv1Ysvvpjp3zIA5HZUMoBc6Ntvv1Xp0qXVoEGDLPXv27ev5s+fr86dO+v5559XTEyMoqKitG/fPi1btsyq7+HDh9W5c2f16dNHPXv21CeffKJevXqpVq1aqly5sjp16iR/f38NGzZMjz32mNq0aZPta9H37Nmjhx56SNWqVdOECRPk5eWlw4cP/+cffT/99JNat26t0qVL69VXX1VycrLee+89NWzYUL///numCU7Xrl1VqlQpRUVF6ffff9dHH32koKAgTZkyJUtxdurUSQMGDNBXX32lp556StKNKkaFChV03333Zep/5MgRLV++XF26dFGpUqUUFxenOXPmqGnTptq7d6/CwsJUsWJFTZgwQa+88oqefvppNW7cWJKsxjIhIUGtW7dWt27d9MQTTyg4OPiW8b377rtau3atevbsqejoaLm7u2vOnDlatWqVFixYoLCwsFu+rmLFilqwYIGGDRume+65x3L5UtGiRZWcnKxmzZrp8OHDGjx4sEqVKqXPP/9cvXr10oULF6wmr5I0d+5cXb16VU8//bS8vLxu+8f2TdeuXdPZs2cl3bhc6o8//tBbb72lJk2aqFSpUpZ+q1ev1pEjR9S7d2+FhIRYLv/bs2ePNm3aZJm0DhgwQF988YUGDx6sSpUqKSEhQRs2bNC+ffssY7R27Vq1bt1atWrV0rhx4+Tm5qa5c+eqefPm+vXXX1W3bt07xnzTY489poULF+r111+XyWTS2bNnLZ/1ypUrM/V/9dVXNX78eEVGRmrgwIE6cOCAZs2apS1btui3336zXAr48ccfq3///mrQoIGGDh2qI0eOqF27dipcuLDuvfdey/HS09PVrl07bdiwQU8//bQqVqyoXbt26e2339bBgwftfvMFAMhRZgC5SmJiolmSuX379lnqv337drMkc9++fa3aX3jhBbMk89q1ay1tJUqUMEsyr1+/3tIWHx9v9vLyMj///POWtqNHj5olmd944w2rY/bs2dNcokSJTDGMGzfOnPHXydtvv22WZD5z5sxt4775HnPnzrW01ahRwxwUFGROSEiwtO3YscPs5uZmfvLJJzO931NPPWV1zI4dO5oDAwNv+54Z8/Dx8TGbzWZz586dzS1atDCbzWZzWlqaOSQkxDx+/PhbfgZXr141p6WlZcrDy8vLPGHCBEvbli1bMuV2U9OmTc2SzLNnz77lvqZNm1q1/fjjj2ZJ5kmTJpmPHDliLliwoLlDhw7/maPZfGO827Zta9X2zjvvmCWZFy5caGlLTU01R0REmAsWLGhOSkqy5CXJ7Ovra46Pj8/y+0nKtDVs2NB89uxZq75XrlzJ9PpPP/000/np5+dnHjRo0G3fMz093VyuXDlzq1atzOnp6VbHL1WqlPmBBx64Y8wZx3n37t1mSeZff/3VbDabze+//765YMGC5suXL1udM2bzjX83np6e5pYtW1qdEzNmzDBLMn/yySdms/nGZxsUFGSuUaOGOSUlxdLvgw8+MEuyGu8FCxaY3dzcLO9/0+zZs82SzL/99pulrUSJEuaePXveMTcAMBKXSwG5TFJSkiSpUKFCWer//fffS5KGDx9u1X7z2+t/r92oVKmS5dt16ca32+Hh4Tpy5Mhdx/xvN9dyfP3110pPT8/Sa06fPq3t27erV69eVt+WV6tWTQ888IAlz4wGDBhg9XPjxo2VkJBg+Qyz4vHHH9cvv/yi2NhYrV27VrGxsbe8VEq6sY7Dze3Gr820tDQlJCRYLgX79+U7d+Ll5aXevXtnqW/Lli3Vv39/TZgwQZ06dZK3t7fmzJmT5ff6t++//14hISF67LHHLG0eHh567rnndOnSJa1bt86q/yOPPKKiRYtm+fj16tXT6tWrtXr1aq1YsUKvvfaa9uzZo3bt2lndySt//vyW///q1as6e/as6tevL0lWn6W/v79iYmJue2nY9u3bLZe3JSQk6OzZszp79qwuX76sFi1aaP369Vk+BytXrqxq1arp008/lXSjqtW+fXsVKFAgU9+ffvpJqampGjp0qOWckKR+/frJ19fX8u9u69atio+P14ABA6zWsvTq1Ut+fn5Wx/z8889VsWJFVahQwZLH2bNn1bx5c0nSzz//nKU8ACA3YJIB5DK+vr6SblyLnhXHjh2Tm5ubypYta9UeEhIif39/HTt2zKq9ePHimY4REBCg8+fP32XEmT366KNq2LCh+vbtq+DgYHXr1k1Lly694x97N+MMDw/PtK9ixYqWPxwz+ncuAQEBkpStXNq0aaNChQrps88+06JFi1SnTp1Mn+VN6enpevvtt1WuXDl5eXmpSJEiKlq0qHbu3KnExMQsv2exYsWytch72rRpKly4sLZv367p06crKCgoy6/9t2PHjqlcuXJWfxhLNz7jm/szyniJU1YUKVJEkZGRioyMVNu2bTV69Gh99NFH2rhxoz766CNLv3PnzmnIkCEKDg5W/vz5VbRoUct7Zfwsp06dqt27d+vee+9V3bp19eqrr1pNiA8dOiTpxvqVokWLWm0fffSRUlJSsjU2jz/+uD7//HMdPnxYGzduvO2E83bnq6enp0qXLm3Zf/P//nvBu4eHh0qXLm3VdujQIe3ZsydTHuXLl5d0Yy0NAOQVrMkAchlfX1+FhYVp9+7d2Xrdvxde3467u/st281m812/R1pamtXP+fPn1/r16/Xzzz/ru+++08qVK/XZZ5+pefPmWrVq1W1jyC5bcrnJy8tLnTp10vz583XkyBG9+uqrt+07efJkjR07Vk899ZQmTpyowoULy83NTUOHDs3yt+WS9bf4WfHHH39Y/sDctWuXVRUip2U31ltp0aKFJGn9+vV69tlnJd1YT7Nx40aNGDFCNWrUUMGCBZWenq4HH3zQ6rPs2rWrGjdurGXLlmnVqlV64403NGXKFH311Vdq3bq1pe8bb7xx29sFZ2dN0WOPPaZRo0apX79+CgwMVMuWLe8y6+xLT09X1apV9dZbb91yf8b1GwCQ2zHJAHKhhx56SB988IGio6MVERFxx74lSpRQenq6Dh06ZPk2WpLi4uJ04cIFy52i7CEgIOCWd87597ffkuTm5qYWLVqoRYsWeuuttzR58mS9/PLL+vnnnxUZGXnLPCTpwIEDmfbt379fRYoUkY+Pj+1J3MLjjz+uTz75RG5uburWrdtt+33xxRe6//779fHHH1u1X7hwQUWKFLH8nNUJX1ZcvnxZvXv3VqVKldSgQQNNnTpVHTt2tNzBKrtKlCihnTt3Kj093aqasX//fst+e7t+/bqkG08gl25UmtasWaPx48frlVdesfS7WZX4t9DQUD3zzDN65plnFB8fr/vuu0+vvfaaWrdurTJlyki6MTm/1XmVXcWLF1fDhg31yy+/aODAgVZ3xMoo4/masSKRmpqqo0ePWmK52e/QoUOWy56kGwvkjx49anW73DJlymjHjh1q0aKFXc8hADACl0sBudCLL74oHx8f9e3bV3FxcZn2//nnn3r33Xcl3bjcR5Leeecdqz43vw1t27at3eIqU6aMEhMTtXPnTkvb6dOnM93B6lZPd775LfPtbsUZGhqqGjVqaP78+VYTmd27d2vVqlWWPHPC/fffr4kTJ2rGjBmW263eiru7e6Yqyeeff66TJ09atd2cDN1qQpZdI0eO1PHjxzV//ny99dZbKlmypHr27HnXtzRt06aNYmNj9dlnn1narl+/rvfee08FCxZU06ZNbY7537799ltJsvxBfbMC9e/P8t/ncFpaWqZLnYKCghQWFmbJv1atWipTpoymTZtmmcRkdPOZL9kxadIkjRs3zlJ1uZXIyEh5enpq+vTpVnl8/PHHSkxMtPy7q127tooWLarZs2crNTXV0m/evHmZzo+uXbvq5MmT+vDDDzO9X3JycqbLBQEgN6OSAeRCZcqU0eLFi/Xoo4+qYsWKVk/83rhxo+WWo9KNP9x69uypDz74QBcuXFDTpk21efNmzZ8/Xx06dND9999vt7i6deumkSNHqmPHjnruued05coVzZo1S+XLl7darDthwgStX79ebdu2VYkSJRQfH6+ZM2fqnnvuUaNGjW57/DfeeEOtW7dWRESE+vTpY7mFrZ+f3x0vY7KVm5tbpic938pDDz2kCRMmqHfv3mrQoIF27dqlRYsWZbq2vkyZMvL399fs2bNVqFAh+fj4qF69etle37B27VrNnDlT48aNs9yude7cuWrWrJnGjh2rqVOnZut4kvT0009rzpw56tWrl7Zt26aSJUvqiy++0G+//aZ33nknyzccuJ2TJ09q4cKFkm58q79jxw7NmTNHRYoUsfzR7uvrqyZNmmjq1Km6du2aihUrplWrVuno0aNWx7p48aLuuecede7cWdWrV1fBggX1008/acuWLXrzzTcl3Ri7jz76SK1bt1blypXVu3dvFStWTCdPntTPP/8sX19fyyQnq5o2bfqfk62iRYtq1KhRGj9+vB588EG1a9dOBw4c0MyZM1WnTh3LAwk9PDw0adIk9e/fX82bN9ejjz6qo0ePau7cuZnOmx49emjp0qUaMGCAfv75ZzVs2FBpaWnav3+/li5dqh9//FG1a9fOVi4AYBhD720F4I4OHjxo7tevn7lkyZJmT09Pc6FChcwNGzY0v/fee+arV69a+l27ds08fvx4c6lSpcweHh7me++91zxq1CirPmbzrW9pajZnvnXq7W5hazabzatWrTJXqVLF7OnpaQ4PDzcvXLgw0y1s16xZY27fvr05LCzM7OnpaQ4LCzM/9thj5oMHD2Z6j3/f5vWnn34yN2zY0Jw/f36zr6+v+eGHHzbv3bvXqs/N9/v3LXLnzp1rlmQ+evTobT9Ts9mc6Xakt3K7W9g+//zz5tDQUHP+/PnNDRs2NEdHR9/y1rNff/21uVKlSuZ8+fJZ5dm0aVNz5cqVb/meGY+TlJRkLlGihPm+++4zX7t2zarfsGHDzG5ububo6Og75nC78Y6LizP37t3bXKRIEbOnp6e5atWqmcbhTufAnd5PGW5d6+bmZg4KCjI/9thj5sOHD1v1PXHihLljx45mf39/s5+fn7lLly7mU6dOmSWZx40bZzabzeaUlBTziBEjzNWrVzcXKlTI7OPjY65evbp55syZmd77jz/+MHfq1MkcGBho9vLyMpcoUcLctWtX85o1a+4Yc1bzvN05M2PGDHOFChXMHh4e5uDgYPPAgQPN58+fz9Rv5syZ5lKlSpm9vLzMtWvXNq9fv/6W501qaqp5ypQp5sqVK5u9vLzMAQEB5lq1apnHjx9vTkxMtPTjFrYAcjuT2ZyNFZIAAAAA8B9YkwEAAADArphkAAAAALArJhkAAAAA7IpJBgAAAAC7YpIBAAAAwK6YZAAAAACwKyYZAAAAAOzKKZ/47dN5rtEhGCJhSW+jQwAAAMgS71z8V2j+moMd9l7Jf8xw2Hs5EpUMAAAAAHaVi+eQAAAAgAFMfA9vKz5BAAAAAHZFJQMAAADIyGQyOoI8j0oGAAAAALuikgEAAABkxJoMm/EJAgAAALArKhkAAABARqzJsBmVDAAAAAB2RSUDAAAAyIg1GTbjEwQAAABgV1QyAAAAgIxYk2EzKhkAAAAA7IpKBgAAAJARazJsxicIAAAAwK6YZAAAAACwKy6XAgAAADJi4bfNqGQAAAAAsCsqGQAAAEBGLPy2GZ8gAAAAALtikpEFBb3zaWqvuto3q4vOLuqhNa+11X1lilj2X/6i9y23oe2qWPqUDfXVZyNb6Ngnj+n0/7pr9cQ2alI5xIh07G7J4kVq/UBz1alZVd27ddGunTuNDskhyJu8ndm2rVv07DMDFNmskapXDtfaNT8ZHZJDuGreN7naeX4TebtW3lliMjluc1JMMrLg/YGNdH/1MPWdvl51n1+uNTtOasUrrRRauIAkqXTfJVbbgPd/VXq6Wcs3/WU5xhejIpXPzaS241eq0Yvfatexc/piVKSC/fMblJV9rPzhe02bGqX+zwzSks+XKTy8ggb276OEhASjQ8tR5E3ezp53cvIVhYeHa9SYcUaH4lCumrfkmue5RN6uljcch0nGf/D2dFeH+iU0ZsFW/bYvTkdiL2ry0u06Epukfi0rSJLiLiRbbW3rFNf6Paf1V/wlSVJgIS+VC/PTm8t3afex8/ozNkmvLNwqH28PVbrX38DsbLdg/lx16txVHTo+ojJly2rMuPHy9vbW8q++NDq0HEXe5O3seTdq3FSDhwxTi8gHjA7FoVw1b8k1z3OJvF0t7ywzuTluc1LOm5md5HMzKZ+7m1KupVm1J6emKaJiUKb+QX7eevC+ezV/zSFLW8LFFB04eUGPNy2jAl755O5mUp+WFRR/IVl/HMm73xhcS03Vvr17VD+igaXNzc1N9es30M4dfxgYWc4ib/J2hbzhWlz1PCdv18objmXo3aXOnj2rTz75RNHR0YqNjZUkhYSEqEGDBurVq5eKFi1qZHiSpEtXr2vTgXiN7Fxd+09cUHziVXVtWEr1yhfVn7EXM/Xv3qysLiZf09cxx6zaHxr/oz4b2UJxC55QutmsM4lX1eG1VbpwOdVRqdjd+QvnlZaWpsDAQKv2wMBAHT16xKCoch55k7fk/HnDtbjqeU7erpV3tjjxWglHMaySsWXLFpUvX17Tp0+Xn5+fmjRpoiZNmsjPz0/Tp09XhQoVtHXr1v88TkpKipKSkqw2c9o1u8bad/p6mWTSnx920/lPn9TANpX0+W9HlW42Z+rbo3k5ffbrn5kqH2/3i9CZxKt6YOz3avrSCn27+Zg+fylSIXl8TQYAAADwb4ZVMp599ll16dJFs2fPlulfs0Wz2awBAwbo2WefVXR09B2PExUVpfHjx1u15avYTp6VOtgt1qNxF/XguB9UwCuffPN7KPZCsuYPa6a/4qwrGQ0qBiu8mL96vvWLVXuzqqFqfd89KtZrsS4m35gAbf8oQc2rF1P3ZmX15vJddovVkQL8A+Tu7p5pkVhCQoKKFClym1flfeRN3pLz5w3X4qrnOXm7Vt7Z4sRrJRzFsE9wx44dGjZsWKYJhiSZTCYNGzZM27dv/8/jjBo1SomJiVabR3jbHIhYupJyXbEXkuXv46nIGmFaseW41f6ezcvp9z/Patex81bt+T1vzOX+XflITzfL5JZ3y3Eenp6qWKmyYjb9MxFMT09XTEy0qlWvaWBkOYu8ydsV8oZrcdXznLxdK284lmGVjJCQEG3evFkVKlS45f7NmzcrODj4P4/j5eUlLy8vqzaTu4ddYrwpsnqYTCaTDp5KVJkQX73Wo7YOnkzUgp//WdxdKL+HOkaU1Kj/bcn0+s0H43X+cqo+GNxYr3++XcmpaeodWV4lgwrqx21/2zVWR+vRs7fGjh6pypWrqErValq4YL6Sk5PVoWMno0PLUeRN3s6e95XLl3X8+D9fpJw8cUL79+2Tn5+fQsPCDIwsZ7lq3pJrnucSebta3llGJcNmhk0yXnjhBT399NPatm2bWrRoYZlQxMXFac2aNfrwww81bdo0o8Kz4lvAU+O711KxQB+dv5Si5ZuOafyn23Q97Z/KROeGpWQymfT5hswLphIupqjDa6v06mO19N2rD8rD3U37/r6gR6euyVT1yGsebN1G58+d08wZ03X27BmFV6iomXM+UqCTl1vJm7ydPe89e3arb+8nLT9PmxolSWrXvqMmTn7dqLBynKvmLbnmeS6Rt6vlDccxmc23WL3sIJ999pnefvttbdu2TWlpNxZKu7u7q1atWho+fLi6du16V8f16TzXnmHmGQlLehsdAgAAQJZ4G3qP0zvLf/9Eh71X8s9jHfZejmTo8D766KN69NFHde3aNZ09e1aSVKRIEXl42PdyJwAAAACOkyvmkB4eHgoNDTU6DAAAAIA1GXbAJwgAAADArphkAAAAALCrXHG5FAAAAJBr3OI5bsgeKhkAAAAA7IpKBgAAAJARC79txicIAAAAwK6oZAAAAAAZsSbDZlQyAAAAANgVlQwAAAAgI9Zk2IxPEAAAAIBdUckAAAAAMmJNhs2oZAAAAACwKyoZAAAAQEasybAZnyAAAAAAu6KSAQAAAGTEmgybUckAAAAAYFdUMgAAAICMWJNhMz5BAAAAAHZFJQMAAADIiDUZNqOSAQAAAMCunLKSkbCkt9EhGCKgzmCjQzDE+S0zjA4BAAA4E9Zk2IxPEAAAAIBdMckAAAAAYFdMMgAAAICMTG6O27Jh/fr1evjhhxUWFiaTyaTly5db7TebzXrllVcUGhqq/PnzKzIyUocOHbLqc+7cOXXv3l2+vr7y9/dXnz59dOnSJas+O3fuVOPGjeXt7a17771XU6dOzfZHyCQDAAAAyAMuX76s6tWr6/3337/l/qlTp2r69OmaPXu2YmJi5OPjo1atWunq1auWPt27d9eePXu0evVqrVixQuvXr9fTTz9t2Z+UlKSWLVuqRIkS2rZtm9544w29+uqr+uCDD7IVq8lsNpvvLs3c6+p1oyMwBgu/AQBAXuGdi28/lL/dLIe9V/I3A+/qdSaTScuWLVOHDh0k3ahihIWF6fnnn9cLL7wgSUpMTFRwcLDmzZunbt26ad++fapUqZK2bNmi2rVrS5JWrlypNm3a6MSJEwoLC9OsWbP08ssvKzY2Vp6enpKkl156ScuXL9f+/fuzHB+VDAAAAMAgKSkpSkpKstpSUlKyfZyjR48qNjZWkZGRljY/Pz/Vq1dP0dHRkqTo6Gj5+/tbJhiSFBkZKTc3N8XExFj6NGnSxDLBkKRWrVrpwIEDOn/+fJbjYZIBAAAAZOTANRlRUVHy8/Oz2qKiorIdcmxsrCQpODjYqj04ONiyLzY2VkFBQVb78+XLp8KFC1v1udUxMr5HVuTiQhUAAADg3EaNGqXhw4dbtXl5eRkUjf0wyQAAAAAyMpkc9lZeXl52mVSEhIRIkuLi4hQaGmppj4uLU40aNSx94uPjrV53/fp1nTt3zvL6kJAQxcXFWfW5+fPNPlnB5VIAAABAHleqVCmFhIRozZo1lrakpCTFxMQoIiJCkhQREaELFy5o27Ztlj5r165Venq66tWrZ+mzfv16Xbt2zdJn9erVCg8PV0BAQJbjYZIBAAAAZJRLn5Nx6dIlbd++Xdu3b5d0Y7H39u3bdfz4cZlMJg0dOlSTJk3SN998o127dunJJ59UWFiY5Q5UFStW1IMPPqh+/fpp8+bN+u233zR48GB169ZNYWFhkqTHH39cnp6e6tOnj/bs2aPPPvtM7777bqZLuv4Ll0sBAAAAecDWrVt1//33W36++Yd/z549NW/ePL344ou6fPmynn76aV24cEGNGjXSypUr5e3tbXnNokWLNHjwYLVo0UJubm565JFHNH36dMt+Pz8/rVq1SoMGDVKtWrVUpEgRvfLKK1bP0sgKnpPhRHhOBgAAyCty9XMyOn3ssPdK/qqPw97LkbhcCgAAAIBd5eI5JAAAAOB4JgfeXcpZUckAAAAAYFdUMgAAAIAMqGTYjkoGAAAAALuikgEAAABkRCHDZlQyAAAAANgVkww72LZ1i559ZoAimzVS9crhWrvmJ6NDyraG95XRF+/015FVryn5jxl6uFk1q/3tm1fXtzMH6cTPU5T8xwxVK1/Man+AbwG9NbKLdiwbq3PRb+ng9xP05oud5VvQ26pfs7rl9fO84YrfME1HV0/WpOfay909b56GSxYvUusHmqtOzarq3q2Ldu3caXRIOerjD+fo8a6PKKJOTTVrHKGhzz6jv44eMTqsHOcM/75twXnuGuf5Ta423jeRt2vlDcfIm3/d5TLJyVcUHh6uUWPGGR3KXfPJ76VdB09qaNRnt9xfIL+nNm7/U2OmL7/l/tCifgot6qdRby9TrS6T1W/cQj3QoJJmj+tu6VO1fDEtf2+gVm3cq/qPva4eL32itk2ratJz7XMipRy18ofvNW1qlPo/M0hLPl+m8PAKGti/jxISEowOLcds3bJZjz7WXQs+Xao5H87V9evXNaBfH125csXo0HKUM/z7vluc565znkuuOd4Sebta3lllMpkctjkrnvhtZ9Urh+vt6e+reYtIh7+3vZ74nfzHDHUd9oG+/SXzNxrFQwvrwPcTVO/RKO08ePKOx+kUWVOfvPakAhs8r7S0dI0f/LBa1K+gRk+8YenTpkkVLZzylIq3GKVLV1LuKl4jnvjdvVsXVa5SVaPHvCJJSk9PV8sWTfXY4z3Up9/TDo/HCOfOndP9jSP0yfyFqlW7jtHhOISR/76NwHnuWue5q443eRuXd25+4nfBrvMc9l6XlvZy2Hs5EpUM5BjfQt5KunxVaWnpkiQvz3y6mnLNqk9yyjXl9/ZUzYrFjQjxrlxLTdW+vXtUP6KBpc3NzU316zfQzh1/GBiZY126eFGS5OvnZ3AkyAmc5ze4ynnuquNN3q6Vd3ZQybAdkwzkiEB/H43q11qffLnR0rZ64z7Vr15aXR+sJTc3k8KK+mn0060lSaFFfY0KNdvOXzivtLQ0BQYGWrUHBgbq7NmzBkXlWOnp6Zo6ZbJq1LxP5cqVNzoc5ADOc9c6z111vMnbtfKGY+XqScbff/+tp5566o59UlJSlJSUZLWlpNzdZTewj0I+3lo2faD2HTmtSXO+s7Sv2bRfo99Zrumjuykx5h3t/PoV/bhhjyQpPd3prtpzapMnjdefhw5p6rS3jQ4FyDGc54DropJhu1w9yTh37pzmz59/xz5RUVHy8/Oz2t6YEuWgCPFvBQt46Zv3n9HFK1f16PAPdf16utX+6QvXKqTJCJVv84ruuf8ly7qPoyfyzjcnAf4Bcnd3z7Q4LiEhQUWKFDEoKseZPGmC1q/7RR/Ona/gkBCjw0EO4Tx3rfPcVcebvF0rbziWoUtuvvnmmzvuP3Lkv28bOGrUKA0fPtyqzezuZVNcuDuFfLz17cxBSkm9rs5D5ygl9fYr8E+fSZQkdX2wtv4+fU5/7P/bUWHazMPTUxUrVVbMpmjLAuD09HTFxESr22NPGBxdzjGbzYp6baLWrlmtj+ct0D333Gt0SMhBnOeudZ676niTt2vlnR3OXGFwFEMnGR06dJDJZNKdbnD1X4Ps5eUlLy/rSYWj7y515fJlHT9+3PLzyRMntH/fPvn5+Sk0LMyxwdwln/yeKnNvUcvPJYsFqlr5YjqfdEV/x55XgG8B3RsSoNCgG4sfy5cMliTFJSQpLuGiCvl4a8XMQcrv7aneL8+Xr4+3fH1uPCPjzPlLlsuhhj3ZQqs27lN6errat6ihF3o/oCde/CTPXS7Vo2dvjR09UpUrV1GVqtW0cMF8JScnq0PHTkaHlmMmTxyvH75foXfemymfAj46e+aMJKlgoULy9vb+j1fnXc7w7/tucZ67znkuueZ4S+TtannDcQy9hW2xYsU0c+ZMtW9/6+ckbN++XbVq1VJaWlq2juvoScaWzTHq2/vJTO3t2nfUxMmvOywOW25h27hWOa36aEim9gXfbNLT4xbqiYfr6cMJPTLtnzT7e7025/vbvl6Swtu8ouOnz0mSfpjzrGpUvFdeHvm06+BJvfbBD1r12967jlsy5ha2kvTpooWaP/djnT17RuEVKmrk6DGqVq26IbE4QvXK4bdsnzApSu2d+D9KueXft1E4z29w9vP8Jlcb75vI25i8c/MtbP0eX+Cw90pcnPnvK2dg6CSjXbt2qlGjhiZMmHDL/Tt27FDNmjWVnp5+y/23Y+RzMoxkr+dk5DVGTTIAAMDdY5Jxg7NOMgwd3hEjRujy5cu33V+2bFn9/PPPDowIAAAAro41GbYzdJLRuHHjO+738fFR06ZNHRQNAAAAAHvIxYUqAAAAwPGoZNguVz8nAwAAAEDeQyUDAAAAyIBKhu2oZAAAAACwKyoZAAAAQAZUMmxHJQMAAACAXVHJAAAAADKikGEzKhkAAAAA7IpJBgAAAAC74nIpAAAAIAMWftuOSgYAAAAAu6KSAQAAAGRAJcN2VDIAAAAA2BWVDAAAACADKhm2o5IBAAAAwK6oZAAAAAAZUciwGZUMAAAAAHZFJQMAAADIgDUZtqOSAQAAAMCuqGQAAAAAGVDJsB2TDCdyfssMo0MwRMBDbxkdgiHOrxhudAgAAAC3xCQDAAAAyIBKhu1YkwEAAADArqhkAAAAABlQybAdlQwAAAAAdkUlAwAAAMiIQobNqGQAAAAAsCsmGQAAAADsisulAAAAgAxY+G07KhkAAAAA7IpKBgAAAJABlQzbUckAAAAAYFdUMgAAAIAMqGTYjkoGAAAAALuikgEAAABkRCHDZlQyAAAAANgVlQwAAAAgA9Zk2I5KBgAAAAC7opIBAAAAZEAlw3ZUMgAAAADYFZUMAAAAIAMqGbajkmFHSxYvUusHmqtOzarq3q2Ldu3caXRIOWrb1i169pkBimzWSNUrh2vtmp+MDinbGlYppi9eba8ji55W8srhejiiTKY+Y3s00JHFT+vc18/pu6hHVCbM37KveLCvZg1rqX3z+ujc189pzydPacwTEfLIZ/1PK7JWCa17+zHFfzVYx5cM0KdjHlbxYN+cTs+unGG874ar5n2Tq/1eu4m8XSNv/n271njDsZhk2MnKH77XtKlR6v/MIC35fJnCwytoYP8+SkhIMDq0HJOcfEXh4eEaNWac0aHcNR9vD+06ekZD3197y/3Pd6mjZ9rX0HPT16jJ0MW6fPWavn2tk7w83CVJ4fcUlptJGjz9J93Xf75e/OAX9W1bTRN6NbIco0Swrz4f116/7DiueoMWqN2YrxTom19Lxj7skBztxRnG+264at6Sa/5ek8jblfLm37drjXd2mEwmh23OikmGnSyYP1edOndVh46PqEzZshozbry8vb21/KsvjQ4txzRq3FSDhwxTi8gHjA7lrq3a+pfGz9+obzYevuX+QR1rasqnMVqx6U/tPnpWfd9YqdDAgmrXoKwkafW2v9T/rVVa8/sx/RWbqO82HdG7X25T+4ZlLce4r1yw3N1MenX+bzp6OlHbD8frnS+3qnrpIOVzzzv/BJ1hvO+Gq+YtuebvNYm8XSlv/n271njDsfLOXzi52LXUVO3bu0f1IxpY2tzc3FS/fgPt3PGHgZHBFiVD/BRauKDW/nHc0pZ0JVVb9seqXsXQ277O18dT5y5etfz8+6E4paeb9WTLKnJzM8m3gKceb1FRa/84putp6TmaA3C3XPX3Gnm7Vt6uivHOApMDNydl+CQjOTlZGzZs0N69ezPtu3r1qv73v//d8fUpKSlKSkqy2lJSUnIq3Fs6f+G80tLSFBgYaNUeGBios2fPOjQW2E9IQAFJUvyFK1bt8RcuKzjA55avKR3qr4Htaurj73dZ2o7FJemhl7/S+F4NlfjtEMV9NVjFihTSE5O/y7ngARu56u818natvF0V4w1HMHSScfDgQVWsWFFNmjRR1apV1bRpU50+fdqyPzExUb17977jMaKiouTn52e1vTElKqdDBzIJCyyob17rpK9+Pai5K/+ZZAQHFNDMIQ9o0U971ei5RYp84TOlXk/T4jEPGRgtAAC4HdZk2M7QScbIkSNVpUoVxcfH68CBAypUqJAaNmyo48eP//eL/9+oUaOUmJhotY0YOSoHo84swD9A7u7umRZLJSQkqEiRIg6NBfYTe/5GBSPIv4BVe5C/j+LOX7ZqCy3so5VTumjT3lMa9O5qq339H66hpCspevnjX7XjzzP6bfdJPTX1BzWvWUJ1K9z+sivASK76e428XStvV8V4wxEMnWRs3LhRUVFRKlKkiMqWLatvv/1WrVq1UuPGjXXkyJEsHcPLy0u+vr5Wm5eXVw5Hbs3D01MVK1VWzKZoS1t6erpiYqJVrXpNh8YC+/krNlGnz13S/TWKW9oKFfBUnQohitn3T8UtLLCgfpzaVX8cjtPTb/0os9n6OAW88ik93box7f9/dnPeLzCQx7nq7zXydq28XRXjDUcw9GF8ycnJypfvnxBMJpNmzZqlwYMHq2nTplq8eLGB0WVPj569NXb0SFWuXEVVqlbTwgXzlZycrA4dOxkdWo65cvmyVdXp5IkT2r9vn/z8/BQaFmZgZFnn4+1h9dyLkiF+qla6qM5fvKq/z1zU+8v+0MjH6unwqfP6KzZJ455soNMJlyx3o7oxweii4/FJGvXhehX1y285Vtz/V0J+2HxUz3aspVGP19fSX/arUAFPje/VSMfiErX9zzMOzdcWzjDed8NV85Zc8/eaRN6ulDf/vl1rvLPDmS9jchST2fzv710dp27dunr22WfVo0ePTPsGDx6sRYsWKSkpSWlpadk67tXr9oowez5dtFDz536ss2fPKLxCRY0cPUbVqlU3JhgH2LI5Rn17P5mpvV37jpo4+XWHxRHw0Ft3/drG1e7RqqldM7UvWL1HT7/5o6QbD+N7qnVV+Rf00sY9JzVkxhodPnlBkvTEA5X04fMP3vLY+R/8J64uTcM1rEttlSsWoCsp1xWz75TGfPyrDp44f9exn18x/K5fezdyy3g7mqvmfZOr/V67ibxdI2/+fRs73t6GftV9Z2We/8Fh7/Xnm60d9l6OZOgkIyoqSr/++qu+//77W+5/5plnNHv2bKWnZ+82n0ZNMmAMWyYZeZmjJxkAANhTbp5klH3BcZOMw9OYZOQZTDJcC5MMAADyHiYZNzjrJCMXDy8AAADgeKzJsJ3hD+MDAAAA4FyoZAAAAAAZUMiwHZUMAAAAAHZFJQMAAADIgDUZtqOSAQAAAMCuqGQAAAAAGVDIsB2VDAAAAAB2RSUDAAAAyMDNjVKGrahkAAAAALArKhkAAABABqzJsB2VDAAAAAB2xSQDAAAAyMBkMjlsy460tDSNHTtWpUqVUv78+VWmTBlNnDhRZrPZ0sdsNuuVV15RaGio8ufPr8jISB06dMjqOOfOnVP37t3l6+srf39/9enTR5cuXbLLZ3cTkwwAAAAgD5gyZYpmzZqlGTNmaN++fZoyZYqmTp2q9957z9Jn6tSpmj59umbPnq2YmBj5+PioVatWunr1qqVP9+7dtWfPHq1evVorVqzQ+vXr9fTTT9s1VtZkAAAAAHnAxo0b1b59e7Vt21aSVLJkSX366afavHmzpBtVjHfeeUdjxoxR+/btJUn/+9//FBwcrOXLl6tbt27at2+fVq5cqS1btqh27dqSpPfee09t2rTRtGnTFBYWZpdYqWQAAAAAGZhMjttSUlKUlJRktaWkpNwyrgYNGmjNmjU6ePCgJGnHjh3asGGDWrduLUk6evSoYmNjFRkZaXmNn5+f6tWrp+joaElSdHS0/P39LRMMSYqMjJSbm5tiYmLs9hkyyQAAAAAMEhUVJT8/P6stKirqln1feukldevWTRUqVJCHh4dq1qypoUOHqnv37pKk2NhYSVJwcLDV64KDgy37YmNjFRQUZLU/X758Kly4sKWPPXC5FAAAAJBBdhdk22LUqFEaPny4VZuXl9ct+y5dulSLFi3S4sWLVblyZW3fvl1Dhw5VWFiYevbs6Yhws4xJBgAAAGAQLy+v204q/m3EiBGWaoYkVa1aVceOHVNUVJR69uypkJAQSVJcXJxCQ0Mtr4uLi1ONGjUkSSEhIYqPj7c67vXr13Xu3DnL6+2By6UAAACADHLrLWyvXLkiNzfrP9/d3d2Vnp4uSSpVqpRCQkK0Zs0ay/6kpCTFxMQoIiJCkhQREaELFy5o27Ztlj5r165Venq66tWrd7cfWSZUMpDnnV8x/L87OaGAjjONDsEQ55c9Y3QIAAAY4uGHH9Zrr72m4sWLq3Llyvrjjz/01ltv6amnnpJ0Y3I0dOhQTZo0SeXKlVOpUqU0duxYhYWFqUOHDpKkihUr6sEHH1S/fv00e/ZsXbt2TYMHD1a3bt3sdmcpiUkGAAAAYMWBSzKy5b333tPYsWP1zDPPKD4+XmFhYerfv79eeeUVS58XX3xRly9f1tNPP60LFy6oUaNGWrlypby9vS19Fi1apMGDB6tFixZyc3PTI488ounTp9s1VpM54yMCncTV60ZHAOQ8KhkAgLzMOxd/1V3j1TX/3clOtr/awmHv5Ui5eHgBAAAAx3Pk3aWcFQu/AQAAANgVlQwAAAAgAwoZtqOSAQAAAMCuqGQAAAAAGbAmw3ZUMgAAAADYFZUMAAAAIAMKGbajkgEAAADArqhkAAAAABmwJsN2VDIAAAAA2BWVDAAAACADChm2o5IBAAAAwK6YZAAAAACwKy6XAgAAADJg4bftqGQAAAAAsCsqGQAAAEAGFDJsRyUDAAAAgF1RyQAAAAAyYE2G7ahkAAAAALArJhl2tGTxIrV+oLnq1Kyq7t26aNfOnUaH5BDknffyblg5VF+MbaMj83oq+dtn9HD9Upn6jO1eR0fm99S5L57WdxMfVplQP6v9AQW9NPf5SMV91lenP+2jWc/eLx/vzMXRoR1raOfsx3Xhq/76c96TerFrrRzLKyfl5fG+G0uXLFbnjg+rQd371KDuferx+KPa8Os6o8NyGFcb75tcLe9tW7fo2WcGKLJZI1WvHK61a34yOiSHcrXxzg6TyXGbs2KSYScrf/he06ZGqf8zg7Tk82UKD6+ggf37KCEhwejQchR55828fbw9tOvoWQ2dvf6W+59/pKaeeaianpu5Tk1e+FKXr17XtxMekpeHu6XP3BciVbF4YT009hs9MvE7NaoSqvcHN7M6zptPN1KvlhU16pONqj5wsTpP/EFbD8blZGo5Iq+P990ICg7RkGEv6NPPv9LipV+qbr36GjJ4kA4fPmR0aDnOFcdbcs28k5OvKDw8XKPGjDM6FIdzxfGGYzHJsJMF8+eqU+eu6tDxEZUpW1Zjxo2Xt7e3ln/1pdGh5Sjyzpt5r9p2XOMXbtY3m47ecv+gdtU0Zek2rYj5S7v/SlDft9cotLCP2v1/xSP8ngC1qlVCz7z3s7YcjNfGvbEaPudXdWlcTqGFC1j69GtdWV0m/aDvNv+lY3EX9cefZ7R2+wmH5WkveX2870az+5urcZOmKlGipEqWLKVnhwxTgQIFtHPHdqNDy3GuON6Sa+bdqHFTDR4yTC0iHzA6FIdzxfHODpPJ5LDNWTHJsINrqanat3eP6kc0sLS5ubmpfv0G2rnjDwMjy1nk7Zx5lwz2VWhhH63d/relLelKqrYcjFO9CiGSpHoVgnX+0lX9fviMpc/a7SeUbjarTvlgSVLbuiV0NDZJbeqU0L6PntD+j57QzGebKaCgl2MTspGzj3dWpKWl6Yfvv1Ny8hVVr17T6HBylKuOt6vm7aoYbziC4XeX2rdvnzZt2qSIiAhVqFBB+/fv17vvvquUlBQ98cQTat68+R1fn5KSopSUFKs2s7uXvLwc94fM+QvnlZaWpsDAQKv2wMBAHT16xGFxOBp5O2feIQE3KhHxF5Kt2uMvJCv4//cFBxTQmX/tT0s369zFq5Y+JUP8VDyokDo1LKO+b62Rm5tJU/s21OKXWqn1mG8ckIl9OPt438mhgwfU4/FuSk1NUYECBfT29PdVpmxZo8PKUa463q6at6tivP+bExcYHMbQSsbKlStVo0YNvfDCC6pZs6ZWrlypJk2a6PDhwzp27JhatmyptWvX3vEYUVFR8vPzs9remBLloAwA3I6bSfL2zKc+b6/Rb3tP69fdpzTwvZ/VrPo9KlfM3+jwkAUlS5bS0i+Xa+GnS9Xl0cc0dvRI/Xn4sNFhAQDyAEMnGRMmTNCIESOUkJCguXPn6vHHH1e/fv20evVqrVmzRiNGjNDrr79+x2OMGjVKiYmJVtuIkaMclMENAf4Bcnd3z7RYKiEhQUWKFHFoLI5E3s6Zd+z5K5KkIP/8Vu1B/vkV9//74s5fUdF/7Xd3M6lwIW9Ln9jzV3TtepoOn0q09Nn/93lJ0r1FC+ZY/Pbm7ON9Jx6enipeooQqVa6iIcOeV/nwClq08H9Gh5WjXHW8XTVvV8V4/zfWZNjO0EnGnj171KtXL0lS165ddfHiRXXu3Nmyv3v37tr5H7dT8/Lykq+vr9XmyEulpBv/Ia5YqbJiNkVb2tLT0xUTE61qTnz9Mnk7Z95/xSXp9LnLur/6PZa2Qvk9VKd8sGL2x0qSYvbHKaCgt2qWKWrp06z6PXIzmbTl/+8eFb0vVh753FUqxNfSp1yYvyTpePxFB2RiH84+3tmRnp6ua6mpRoeRo1x1vF01b1fFeMMRDF+TcXMG5+bmJm9vb/n5/XMv/kKFCikxMfF2L81VevTsrbGjR6py5SqqUrWaFi6Yr+TkZHXo2Mno0HIUeefNvH2881k996JkcCFVKxWo85dS9PeZS3r/m50a+WgtHT6VqL/ikjTuibo6fe6y5W5UB06c14/bjun9Z5vpuffXySOfm97u31if/3pIp8/dqGSs3f63fj8crzlD7teID3+Tm8mkdwY01k9//G1V3cgL8vp43413335TjRo3UUhoqK5cvqzvv1uhrVs2a9YHHxsdWo5zxfGWXDPvK5cv6/jx45afT544of379snPz0+hYWEGRpbzXHG8s8OZKwyOYugko2TJkjp06JDKlCkjSYqOjlbx4sUt+48fP67Q0FCjwsuWB1u30flz5zRzxnSdPXtG4RUqauacjxTo5GVH8s6bed9XNkirojpYfp7at5EkacGa/Xr6nbV688s/VMA7n2YMbiZ/H09t3Hta7catUMq1NMtrek/7SW8PaKzvJ7VTutms5RuP6PkPfrXsN5ulzhO/11v9G2t1VAddTrmuVduO6aWPNzosT3vJ6+N9N86dS9CYUSN15ky8ChYqpPLlwzXrg48V0aCh0aHlOFccb8k1896zZ7f69n7S8vO0qTfWdLZr31ETJ9/5cu28zhXHG45lMpvNZqPefPbs2br33nvVtm3bW+4fPXq04uPj9dFHH2XruFev2yM6IHcL6DjT6BAMcX7ZM0aHAACwA2/Dr6e5vaZv/+aw91o3zDm/vDF0eAcMGHDH/ZMnT3ZQJAAAAADshYfxAQAAALCrXFyoAgAAAByPhd+2o5IBAAAAwK6oZAAAAAAZUMiwHZUMAAAAAHZFJQMAAADIgDUZtqOSAQAAAMCuqGQAAAAAGVDIsB2VDAAAAAB2RSUDAAAAyMCNUobNqGQAAAAAsCsqGQAAAEAGFDJsRyUDAAAAgF1RyQAAAAAy4DkZtqOSAQAAAMCuqGQAAAAAGbhRyLAZlQwAAAAAdkUlAwAAAMiANRm2o5IBAAAAwK6oZAAAAAAZUMiwHZMMII86v+wZo0MwROFHPzE6BEOc++wpo0MAACDLuFwKAAAAgF1RyQAAAAAyMInrpWxFJQMAAACAXVHJAAAAADLgYXy2o5IBAAAAwK6oZAAAAAAZ8DA+21HJAAAAAGBXVDIAAACADChk2I5KBgAAAAC7opIBAAAAZOBGKcNmVDIAAAAA2BWVDAAAACADChm2o5IBAAAAwK6oZAAAAAAZ8JwM21HJAAAAAGBXVDIAAACADChk2I5KBgAAAAC7opIBAAAAZMBzMmxHJQMAAACAXTHJAAAAAGBXXC4FAAAAZMDFUrajkmFHSxYvUusHmqtOzarq3q2Ldu3caXRIDkHe5J2XFfTOp6m962n/7K5KWPyk1r7WVrXKFLHs9/HOp7f61tehDx5VwuInte2djurbMtzqGE89EK6V41srdsETuvLlU/Ir4OnoNHKMs413VpE3eTuzbVu36NlnBiiyWSNVrxyutWt+MjokOCEmGXay8ofvNW1qlPo/M0hLPl+m8PAKGti/jxISEowOLUeRN3nn9bxnPtNIzauHqc/0daozfJnW7DilFeMeVFjhApKkKb3q6YEa9+ipd9ep5pCv9P53e/VW3wi1rX2v5Rj5Pd21evtJvfGVc/1h4ozjnRXkTd7Onndy8hWFh4dr1JhxRoeSa5lMJodtzirXTTLMZrPRIdyVBfPnqlPnrurQ8RGVKVtWY8aNl7e3t5Z/9aXRoeUo8ibvvJy3t6e7OtQvqTH/26Lf9sbpSOxFvbb0Dx2JTVK/VhUkSfXCg7Tol0P6dU+sjp+5pE9WH9Cuv86pdrmiluO8/91evblspzYfjDcqlRzhbOOdVeRN3s6ed6PGTTV4yDC1iHzA6FDgxHLdJMPLy0v79u0zOoxsuZaaqn1796h+RANLm5ubm+rXb6CdO/4wMLKcRd7kndfzzudmUj53N129lmbVnpyapogKwZKkmAPxalunuKWy0aRKiMqG+emnHScdHq8jOeN4ZwV5k7cr5I3/5mZy3OasDFv4PXz48Fu2p6Wl6fXXX1dgYKAk6a233rrjcVJSUpSSkmLVZnb3kpeXl30CzYLzF84rLS3NEvNNgYGBOnr0iMPicDTyJm8pb+d96ep1bdofp5c619CBExcUl3hVXRuVVr3yRfVn7EVJ0vCPojVjQEMd/rCbrl1PV7rZrEGzftNve+MMjj5nOeN4ZwV5k7fk/HkDjmDYJOOdd95R9erV5e/vb9VuNpu1b98++fj4ZOk6taioKI0fP96q7eWx4zTmlVftGC0AZ9Vn+nrNHtRIf370mK6npWv7kQQt3XBENf9/8ffANpVUt3yQOket1vEzl9SoUoje7heh0+ev6OedpwyOHgCQE5x5rYSjGDbJmDx5sj744AO9+eabat68uaXdw8ND8+bNU6VKlbJ0nFGjRmWqipjdHVfFkKQA/wC5u7tnWiSWkJCgIkWK3OZVeR95k7eU9/M+GndRrV75QQW88sk3v4diLyTrf8Ob6a+4i/L2dNf4x2up29Q1Wvn7CUnS7mPnVa1kYQ1tV8WpJxnOOt7/hbzJW3L+vAFHMGxNxksvvaTPPvtMAwcO1AsvvKBr167d1XG8vLzk6+trtTnyUilJ8vD0VMVKlRWzKdrSlp6erpiYaFWrXtOhsTgSeZO3M+V9JeW6Yi8ky9/HU5E1imnFluPycHeTp4e70v91P4q0dLPTf8vl7ON9O+RN3q6QN/6byeS4zVkZ+jC+OnXqaNu2bRo0aJBq166tRYsW5dn/cPfo2VtjR49U5cpVVKVqNS1cMF/Jycnq0LGT0aHlKPIm77yed2SNYjJJOngqUWVCfDX5yTo6eDJR/1t7UNfTzFq/+7Ree7KOklOv6/iZS2pcOUSPNy2rl+Zvthwj2D+/gv3zq0yIrySpcokAXUq+pr/PXtL5S6kGZWY7ZxzvrCBv8nb2vK9cvqzjx49bfj554oT279snPz8/hYaFGRgZnInhT/wuWLCg5s+fryVLligyMlJpaWn//aJc6MHWbXT+3DnNnDFdZ8+eUXiFipo55yMFOnm5lbzJO6/n7VvAUxO611KxQB+dv5Si5Zv+0quLt+l62o3yRc+3f9GE7rU0d0hTBRT00vGzl/Tqp9v04Y/7Lcfo27KCXn70n289f5rUVpL09Iz1WvjzYccmZEfOON5ZQd7k7ex579mzW317P2n5edrUKElSu/YdNXHy60aFlavk1S+9cxOTORc9mOLEiRPatm2bIiMj5ePjc9fHuXrdjkEByFUKP/qJ0SEY4txnTxkdAgDYlbfhX3Xf3pOLHfdw1f89Xs1h7+VIueo5Gffcc4/at29v0wQDAAAAsEVufk7GyZMn9cQTTygwMFD58+dX1apVtXXrVst+s9msV155RaGhocqfP78iIyN16NAhq2OcO3dO3bt3l6+vr/z9/dWnTx9dunTJ1o/NSq6aZAAAAAC4tfPnz6thw4by8PDQDz/8oL179+rNN99UQECApc/UqVM1ffp0zZ49WzExMfLx8VGrVq109epVS5/u3btrz549Wr16tVasWKH169fr6aeftmusuepyKXvhcinAeXG5FAA4h9x8uVTvJbsc9l5zu1XNct+XXnpJv/32m3799ddb7jebzQoLC9Pzzz+vF154QZKUmJio4OBgzZs3T926ddO+fftUqVIlbdmyRbVr15YkrVy5Um3atNGJEycUZqfF/1QyAAAAAIOkpKQoKSnJaktJSbll32+++Ua1a9dWly5dFBQUpJo1a+rDDz+07D969KhiY2MVGRlpafPz81O9evUUHX3jVs3R0dHy9/e3TDAkKTIyUm5uboqJibFbXkwyAAAAgAxMDtyioqLk5+dntUVFRd0yriNHjmjWrFkqV66cfvzxRw0cOFDPPfec5s+fL0mKjY2VJAUHB1u9Ljg42LIvNjZWQUFBVvvz5cunwoULW/rYQy4uVAEAAADObdSoURo+fLhV2+0eLJ2enq7atWtr8uTJkqSaNWtq9+7dmj17tnr27JnjsWYHlQwAAAAgAzeTyWGbl5eXfH19rbbbTTJCQ0NVqVIlq7aKFStaHq4YEhIiSYqLi7PqExcXZ9kXEhKi+Ph4q/3Xr1/XuXPnLH3sgUkGAAAAkAc0bNhQBw4csGo7ePCgSpQoIUkqVaqUQkJCtGbNGsv+pKQkxcTEKCIiQpIUERGhCxcuaNu2bZY+a9euVXp6uurVq2e3WLlcCgAAAMgDhg0bpgYNGmjy5Mnq2rWrNm/erA8++EAffPCBpBtPKh86dKgmTZqkcuXKqVSpUho7dqzCwsLUoUMHSTcqHw8++KD69eun2bNn69q1axo8eLC6detmtztLSXdZyfj111/1xBNPKCIiQidPnpQkLViwQBs2bLBbYAAAAIARTCbHbdlRp04dLVu2TJ9++qmqVKmiiRMn6p133lH37t0tfV588UU9++yzevrpp1WnTh1dunRJK1eulLe3t6XPokWLVKFCBbVo0UJt2rRRo0aNLBMVe8n2czK+/PJL9ejRQ927d9eCBQu0d+9elS5dWjNmzND333+v77//3q4B3g2ekwE4L56TAQDOITc/J6Pf0t0Oe68Pu1Zx2Hs5UrYrGZMmTdLs2bP14YcfysPDw9LesGFD/f7773YNDgAAAHA0k8nksM1ZZXuSceDAATVp0iRTu5+fny5cuGCPmAAAAADkYdmeZISEhOjw4cOZ2jds2KDSpUvbJSgAAADAKLl1TUZeku1JRr9+/TRkyBDFxMTIZDLp1KlTWrRokV544QUNHDgwJ2IEAAAAkIdke8nNSy+9pPT0dLVo0UJXrlxRkyZN5OXlpRdeeEHPPvtsTsQIAAAAOIybM5cYHCTbkwyTyaSXX35ZI0aM0OHDh3Xp0iVVqlRJBQsWzIn4AAAAAOQxd33zME9Pz0yPNQcAAADyOgoZtsv2JOP++++/4+221q5da1NAAAAAAPK2bE8yatSoYfXztWvXtH37du3evVs9e/a0V1wAAACAIZz5+RWOku1Jxttvv33L9ldffVWXLl2yOSAAAAAAeZvJbDab7XGgw4cPq27dujp37pw9DmeTq9eNjgAA7Kv04K+MDsEQR2Z0MjoEADnE+65XBue8Z5ftc9h7vdexosPey5Gy/ZyM24mOjpa3t7e9DgcAAAAgj8r2HLJTJ+tvlcxms06fPq2tW7dq7NixdgsMAAAAMAJrMmyX7UmGn5+f1c9ubm4KDw/XhAkT1LJlS7sFBgAAACBvytYkIy0tTb1791bVqlUVEBCQUzEBAAAAhnGjkGGzbK3JcHd3V8uWLXXhwoUcCgcAAABAXpfthd9VqlTRkSNHciIWAAAAAE4g25OMSZMm6YUXXtCKFSt0+vRpJSUlWW0AAABAXuZmctzmrLK8JmPChAl6/vnn1aZNG0lSu3btrFbem81mmUwmpaWl2T9KAAAAAHlGlicZ48eP14ABA/Tzzz/nZDwAAACAobiFre2yPMm4+WDwpk2b5lgwAAAAAPK+bN3CllkdAAAAnJ0zr5VwlGxNMsqXL/+fE41z587ZFBAAAACAvC1bk4zx48dneuI3AAAA4Ey4eMd22ZpkdOvWTUFBQTkVCwAAAAAnkOVJBusxAAAA4Arc+LvXZll+GN/Nu0sBAAAAwJ1kuZKRnp6ek3EAAAAAuUKWv4XHbfEZAgAAALCrbC38BgAAAJwdSzJsRyUDAAAAgF1RyQAAAAAy4O5StqOSYUdLFi9S6weaq07NqurerYt27dxpdEgOQd6ukfe2rVv07DMDFNmskapXDtfaNT8ZHZJDOdN4u5mkEQ9X0qZJrfTn9PbaOLGlhrapYNWngJe7XutWXVujWuvP6e31y7hI9WhcyqpPiSI++nhAfe16o60OvP2wZverqyKFvByZit1xnjvPeZ4drpa3q5/ncAwmGXay8ofvNW1qlPo/M0hLPl+m8PAKGti/jxISEowOLUeRt+vknZx8ReHh4Ro1ZpzRoTics433oFbh6tm0lF5eskNNx6/Wa8t265mW5dTn/jKWPq92rqZmlYL17Nwtajp+tT5cc1ivdauultVCJUn5Pd316ZCGMpvN6vL2r2r/xjp5urtp/qCIPH0tM+e585znWeWKebvyeZ5VJpPjNmfFJMNOFsyfq06du6pDx0dUpmxZjRk3Xt7e3lr+1ZdGh5ajyNt18m7UuKkGDxmmFpEPGB2KwznbeNcuXVg/7jitNbtjdSLhir77/ZTW7Y1XjZIBVn0+33Rc0QfP6kTCFS3a8Jf2nki09KlbJlD3Bvpo6Pxt2n8qSftPJWnIvK2qXjxAjcKLGpWazTjPnec8zypXzNuVz3M4DpMMO7iWmqp9e/eofkQDS5ubm5vq12+gnTv+MDCynEXerpW3q3LG8d565JwaVSiq0kEFJUmVivmpbtlArd0TZ9WnZbVQhfh7S5IalC+i0sEFtW7vjT6e+dxkNpuVev2fZyilXE9XutmsumWLODAb2IMznudZ4ap547+5mRy3OatctfD78uXLWrp0qQ4fPqzQ0FA99thjCgwMvONrUlJSlJKSYtVmdveSl5fjrgs+f+G80tLSMsUaGBioo0ePOCwORyNv18rbVTnjeM/48YAKeefT+lcfUJrZLHeTSa9/vUfLNv9t6TPmsx2a2r2mfn+9ja6lpSs93awRC/9QzOEbl5BsO3pOV1LT9HLHKnp9+R7JJL3csYryubspyNfbqNRwl5zxPM8KV80bcARDKxmVKlXSuXPnJEl///23qlSpomHDhmn16tUaN26cKlWqpKNHj97xGFFRUfLz87Pa3pgS5YjwASBPalfrHnWqe68GfbJFrV5bqyHzt2rAA+XUpX5xS5+n7i+jWqUKq+f7G/Xg5LWa8OUuTX6suhpXuHEp1LlLqer/QYweqBaiQ++204G3H5Zvfg/tPHZe6WazUakBAHIJQysZ+/fv1/Xr1yVJo0aNUlhYmLZv3y4/Pz9dunRJHTt21Msvv6zFixff9hijRo3S8OHDrdrM7o69u0mAf4Dc3d0zLRJLSEhQkSLOe9kAebtW3q7KGcd7bKcqmvHjQX299YQkaf+pJN1TuICefTBcn286Lm8PN73UvrL6zN6kNbtjJUn7Tiap8j3+GvBAef26/4wkad2+eDUYu0qFfTx1Pd2spORr2j6ljY6fPWFYbrg7znieZ4Wr5o3/xi1sbZdr1mRER0fr1VdflZ+fnySpYMGCGj9+vDZs2HDH13l5ecnX19dqc+SlUpLk4empipUqK2ZTtKUtPT1dMTHRqla9pkNjcSTydq28XZUzjre3p3umakNautlyl5N87m7yzOd2yz63un743OVUJSVfU8PwoipSyEurdp7OqdCRQ5zxPM8KV80bcATD12SY/v+/alevXlVoaKjVvmLFiunMmTNGhJVtPXr21tjRI1W5chVVqVpNCxfMV3Jysjp07GR0aDmKvF0n7yuXL+v48eOWn0+eOKH9+/bJz89PoWFhBkaW85xtvFfvitVzrSvo5LlkHTidpCr3+qt/ZDkt2fiXJOnS1evaePCMxnaqoqvX0nQi4YoiyhdR5/rFNf6Lf54f8GhECR2KTVLCxVTVKl1YE7pW0wdrDuvPuEsGZWY7znPnOc+zyhXzduXzPKsoZNjO8ElGixYtlC9fPiUlJenAgQOqUqWKZd+xY8f+c+F3bvFg6zY6f+6cZs6YrrNnzyi8QkXNnPORAp283ErerpP3nj271bf3k5afp029sfapXfuOmjj5daPCcghnG+8xS3boxXaVFPVYDQUW8lJcYrIW/HpUb3+3z9Jn4EebNbpDFc14qo78C3jq5LkrmvL1Hv1v/T/r5MoEF9SoDpXl7+OpvxMua/oPB/TBmsNGpGQ3nOfOc55nlSvm7crnORzHZDYbt0Jv/PjxVj/Xr19frVq1svw8YsQInThxQp9++mm2jnv1ul3CA4Bco/Tgr4wOwRBHZjjvt8mAq/M2/Kvu23vNgV+YvNyirMPey5EMHd5x4+78pMk33njDQZEAAAAAsJdcPIcEAAAAHM8kFmXYKtfcXQoAAACAc6CSAQAAAGRwq9t1I3uoZAAAAACwKyoZAAAAQAZUMmxHJQMAAACAXVHJAAAAADIw8chvm1HJAAAAAGBXVDIAAACADFiTYTsqGQAAAADsikoGAAAAkAFLMmxHJQMAAACAXTHJAAAAAGBXXC4FAAAAZODG9VI2o5IBAAAAwK6oZAAAAAAZcAtb21HJAAAAAGBXVDIAAACADFiSYTsqGQAAAADsikoGAAAAkIGbKGXYikkGAOQBR2Z0MjoEQwQ/ucDoEAwR978eRocAADZhkgEAAABkwJoM27EmAwAAAIBdUckAAAAAMuA5GbajkgEAAADArqhkAAAAABm4sSjDZlQyAAAAANgVlQwAAAAgAwoZtqOSAQAAAMCuqGQAAAAAGbAmw3ZUMgAAAADYFZUMAAAAIAMKGbajkgEAAADArphkAAAAALArLpcCAAAAMuBbeNvxGQIAAACwKyoZAAAAQAYmVn7bjEoGAAAAALuikgEAAABkQB3DdlQyAAAAANgVlQwAAAAgAzfWZNiMSgYAAACQx7z++usymUwaOnSope3q1asaNGiQAgMDVbBgQT3yyCOKi4uzet3x48fVtm1bFShQQEFBQRoxYoSuX79u9/iYZOSAjz/8QNUrh2tq1GtGh5KjPv5wjh7v+ogi6tRUs8YRGvrsM/rr6BGjw3KYJYsXqfUDzVWnZlV179ZFu3buNDokhyBv18h729YtevaZAYps1kjVK4dr7ZqfjA7JLgp651NUj9ra9W5Hxc57TKtebaX7Sgda9s/s30CJi3tYbV+ObG51jAAfT304qJH+/uhRHfvwUc3oFyEfL+e4MIDz3DnO86xytfHODpMDt7uxZcsWzZkzR9WqVbNqHzZsmL799lt9/vnnWrdunU6dOqVOnTpZ9qelpalt27ZKTU3Vxo0bNX/+fM2bN0+vvPLKXUZye0wy7Gz3rp364vMlKl8+3OhQctzWLZv16GPdteDTpZrz4Vxdv35dA/r10ZUrV4wOLcet/OF7TZsapf7PDNKSz5cpPLyCBvbvo4SEBKNDy1Hk7Tp5JydfUXh4uEaNGWd0KHb1Xr8I3V81VP1n/aYGI1do7a7TWj46UqEB+S19Vm8/qXIDP7dsfWZssDrGh4MaqUIxP3WIWqNHp61Vg4pBerdvfUenYnec567FFcfbWVy6dEndu3fXhx9+qICAAEt7YmKiPv74Y7311ltq3ry5atWqpblz52rjxo3atGmTJGnVqlXau3evFi5cqBo1aqh169aaOHGi3n//faWmpto1TiYZdnTl8mWNGjlC48ZPkq+fn9Hh5LhZH3ys9h07qWzZcgqvUEETXntdp0+f0r69e4wOLcctmD9XnTp3VYeOj6hM2bIaM268vL29tfyrL40OLUeRt+vk3ahxUw0eMkwtIh8wOhS78fZwV7u6xfXK4t+1cX+8jsRd1Otf7tTRuIvqE/nPF0Mp19MVn3jVsl24/M9/eMuH+eqBGsX03IfR2vbnWW06cEYj5m3RIxElFeKf/1Zvm2dwnrsWVxzv7DCZHLelpKQoKSnJaktJSbltbIMGDVLbtm0VGRlp1b5t2zZdu3bNqr1ChQoqXry4oqOjJUnR0dGqWrWqgoODLX1atWqlpKQk7dlj37/fmGTY0eRJE9SkSVPVj2hgdCiGuHTxoiQ5/QTrWmqq9u3dYzXObm5uql+/gXbu+MPAyHIWebtW3s4on7tJ+dzdlHItzao9OTVN9cOLWn5uVDFYh2d10dZp7fTWU3UVUNDTsq9uuaK6cDlFfxw9Z2n7ZfdppZvNql22SM4nkUM4z10L4527REVFyc/Pz2qLioq6Zd8lS5bo999/v+X+2NhYeXp6yt/f36o9ODhYsbGxlj4ZJxg399/cZ0+GTjJ+//13HT161PLzggUL1LBhQ917771q1KiRlixZ8p/HyO7sL6f88P132rdvr54b9rzD3zs3SE9P19Qpk1Wj5n0qV6680eHkqPMXzistLU2BgYFW7YGBgTp79qxBUeU88natvJ3RpavXFXMwXiM6VlWIf365mUzq2rCU6pYrYqlCrNl5SgNm/aZ2k1dr3JI/1LBCsL4c2cJyp5lg//w6k3jV6rhp6Wadv5Sq4DxcyeA8dy2M938zmUwO20aNGqXExESrbdSoUZli+vvvvzVkyBAtWrRI3t7eBnwq2WPoJKN37976888/JUkfffSR+vfvr9q1a+vll19WnTp11K9fP33yySd3PMatZn9vTLn17C+nxJ4+ramvv6aoKW/Iy8vLoe+dW0yeNF5/HjqkqdPeNjoUALit/jN/k8lk0oGZnXXmf49rwIMV9MXGv5RuvrH/y+i/9MPvJ7T37wv6buvfenTaz6pVpogaVwq+84EB4C55eXnJ19fXarvV35Pbtm1TfHy87rvvPuXLl0/58uXTunXrNH36dOXLl0/BwcFKTU3VhQsXrF4XFxenkJAQSVJISEimu03d/PlmH3sx9HYYhw4dUrly5SRJM2fO1Lvvvqt+/fpZ9tepU0evvfaannrqqdseY9SoURo+fLhVm9ndsX/o7927R+cSEtSti/Xq/W1bt2jJp4u05Y9dcnd3d2hMjjR50gStX/eLPpm/UMF2PkFzowD/ALm7u2daHJeQkKAiRfLu5RL/hbxdK29ndTT+ktpOXKUCXvlUKL+H4i4ka+6zjfVX/MVb9v8r/pLOJl1V6eBCWrcnVnEXklXUz/obRHc3kwIKeiruQrIjUsgRnOeuhfH+b7lxPUGLFi20a9cuq7bevXurQoUKGjlypO699155eHhozZo1euSRRyRJBw4c0PHjxxURESFJioiI0Guvvab4+HgFBQVJklavXi1fX19VqlTJrvEa+hkWKFDAUpY7efKk6tata7W/Xr16VpdT3UpWZ385qV79+vpi+bf67Mvllq1y5Spq89DD+uzL5U47wTCbzZo8aYLWrlmtDz+Zr3vuudfokBzCw9NTFStVVsymaEtbenq6YmKiVa16TQMjy1nk7Vp5O7srKdcVdyFZ/j6eal4tTN9vO3HLfmGFC6hwQS/F/v8EYvOhM/L38VKNUoUtfZpWDpGbyaSth/PuZSac566F8c6bChUqpCpVqlhtPj4+CgwMVJUqVeTn56c+ffpo+PDh+vnnn7Vt2zb17t1bERERql//xh3wWrZsqUqVKqlHjx7asWOHfvzxR40ZM0aDBg2y+9/PhlYyWrdurVmzZumjjz5S06ZN9cUXX6h69eqW/UuXLlXZsmUNjDBrfHwKZlqHkL9AAfn7+Tv1+oTJE8frh+9X6J33ZsqngI/OnjkjSSpYqFCeuFbQFj169tbY0SNVuXIVValaTQsXzFdycrI6dOz03y/Ow8jbdfK+cvmyjh8/bvn55IkT2r9vn/z8/BQaFmZgZLZpUS1UkkmHTyepdHAhTXj8Ph06laiF6w7LxyufXnqkmr7efFzxF5JV6v/3H4m7qDU7T0mSDp5K0urtJzW9b30N/SRGHu5ueqNXXX0Z/ZdlIpJXcZ47z3meFa443tlhyqNP/H777bfl5uamRx55RCkpKWrVqpVmzpxp2e/u7q4VK1Zo4MCBioiIkI+Pj3r27KkJEybYPRaT2Ww22/2oWXTq1Ck1bNhQxYsXV+3atTVr1izVqlVLFStW1IEDB7Rp0yYtW7ZMbdq0ydZxr9r/oYXZ1qdXD4WHV9CLo142OpQcU73yrZ8FMmFSlNq7wC+pTxct1Py5H+vs2TMKr1BRI0ePUbVq1f/7hXkcebtG3ls2x6hv7ycztbdr31ETJ7/usDiCn1xg1+N1rFdC47rVVFjhAjp/KUXfbDmuiZ9tV1LyNXl7uGvx881UrURh+fl46PT5ZP2867QmLd2uM0n/LPYO8PHUG73q6sH77lG62axvNh/XyPlbdDnFfv/xiftfD7sdKzs4z29w9HluFKPH2zsXP8Ny6fZTDnuvrjWcc0Jr6CRDki5cuKDXX39d3377rY4cOaL09HSFhoaqYcOGGjZsmGrXrp3tY+aGSQYAwHb2nmTkFUZNMgBHYpJxA5OMPIRJBgA4ByYZgPPKzZOMzx04yejipJOM3Lh4HgAAAEAelovnkAAAAIDj5dWF37kJlQwAAAAAdkUlAwAAAMiAb+Ftx2cIAAAAwK6oZAAAAAAZsCbDdlQyAAAAANgVlQwAAAAgA+oYtqOSAQAAAMCuqGQAAAAAGbAkw3ZUMgAAAADYFZUMAAAAIAM3VmXYjEoGAAAAALuikgEAAABkwJoM21HJAAAAAGBXVDIAAACADEysybAZlQwAAAAAdkUlAwAAAMiANRm2o5IBAAAAwK6YZAAAAACwKy6XAgDkWnH/62F0CIYIaPeu0SEY4vw3Q4wOAZDEw/jsgUoGAAAAALuikgEAAABkwMJv21HJAAAAAGBXVDIAAACADKhk2I5KBgAAAAC7opIBAAAAZGDi7lI2o5IBAAAAwK6oZAAAAAAZuFHIsBmVDAAAAAB2RSUDAAAAyIA1GbajkgEAAADArqhkAAAAABnwnAzbUckAAAAAYFdUMgAAAIAMWJNhOyoZAAAAAOyKSgYAAACQAc/JsB2VDAAAAAB2xSQDAAAAgF1xuRQAAACQAQu/bUclAwAAAIBdUckAAAAAMuBhfLajkmEH27Zu0bPPDFBks0aqXjlca9f8ZHRIDkHerpX3TUsWL1LrB5qrTs2q6t6ti3bt3Gl0SDlq6ZLF6tzxYTWoe58a1L1PPR5/VBt+XWd0WA7jauN9U17Ou2GVMH0x7mEdWdBHyd8P0cMRpTP1GftEfR1Z2Ffnlg3Sd691VJkwf6v9Lz5aRz9P66KEr57R6aUDbvk+zarfq5+ndVH8FwN1dGFfTerdUO559JY8eXm874ar/16DYzDJsIPk5CsKDw/XqDHjjA7FocjbtfKWpJU/fK9pU6PU/5lBWvL5MoWHV9DA/n2UkJBgdGg5Jig4REOGvaBPP/9Ki5d+qbr16mvI4EE6fPiQ0aHlOFccbynv5+3j7aFdR89q6Mxfbrn/+c619Ey7Gnpuxlo1GfaZLl+9pm8ndpCXh7ulj2c+d3214bA+/H7XLY9RtVQRLZ/QTqu2HVP9Zxerx+s/qG290prUu2FOpJSj8vp43w1X/r2WVSYHbs6KSYYdNGrcVIOHDFOLyAeMDsWhyNu18pakBfPnqlPnrurQ8RGVKVtWY8aNl7e3t5Z/9aXRoeWYZvc3V+MmTVWiREmVLFlKzw4ZpgIFCmjnju1Gh5bjXHG8pbyf96qtxzT+f9H6JvrPW+4f1KGmpizZrBWbjmj3X2fV981VCg30UbuIMpY+kxZt0nvL/9Duv87e8hidm5TX7qMJivp0s46cTtSG3Sf18icb1P+h6iqY3yNH8sopeX2874Yr/16D4zDJAJAl11JTtW/vHtWPaGBpc3NzU/36DbRzxx8GRuY4aWlp+uH775ScfEXVq9c0Opwc5arj7ex5lwzxVWhhH63dftzSlnQlVVsOxKpexZAsH8fLw11XU69btSWnXld+r3yqWTbIbvHmNGcf76xwpd9r2eFmMjlsc1aGLvx+9tln1bVrVzVu3Piuj5GSkqKUlBSrNrO7l7y8vGwND0AG5y+cV1pamgIDA63aAwMDdfToEYOicoxDBw+ox+PdlJqaogIFCujt6e+rTNmyRoeVo1x1vJ0975AAH0lS/PkrVu3xF64o+P/3ZcXqbcc0uH0NdW1aXl/8ekghAQU0+vF6kqTQwlk/jtGcfbzvxBV/r8GxDK1kvP/++2rWrJnKly+vKVOmKDY2NtvHiIqKkp+fn9X2xpSoHIgWgKsqWbKUln65XAs/Xaoujz6msaNH6s/Dh40OCzDMmj+Oa/QnGzR9cHMlfj1YOz/sqR+3/CVJSjebjQ0OWcLvtTtjTYbtDL9catWqVWrTpo2mTZum4sWLq3379lqxYoXS09Oz9PpRo0YpMTHRahsxclQORw24ngD/ALm7u2daDJmQkKAiRYoYFJVjeHh6qniJEqpUuYqGDHte5cMraNHC/xkdVo5y1fF29rxjz1+WJAUFFLBqD/IvoLj/35dV05f9oZAus1W+5ye6p9scfbvpxhqQo6eT7BOsAzj7eN+JK/5eg2MZPsmoWrWq3nnnHZ06dUoLFy5USkqKOnTooHvvvVcvv/yyDv/HrNrLy0u+vr5WG5dKAfbn4empipUqK2ZTtKUtPT1dMTHRquZi1/Gmp6frWmqq0WHkKFcdb2fP+6/YJJ0+d1n3V7/X0lYov6fqhIcoZl/2ryaQpNPnLutqapq6Ng3X3/EX9cef8fYKN8c5+3hnhyv8XssWShk2yzUP4/Pw8FDXrl3VtWtXHT9+XJ988onmzZun119/XWlpaUaHd0dXLl/W8eP/LKI7eeKE9u/bJz8/P4WGhRkYWc4i7xtcJW9J6tGzt8aOHqnKlauoStVqWrhgvpKTk9WhYyejQ8sx7779pho1bqKQ0FBduXxZ33+3Qlu3bNasDz42OrQc54rjLeX9vH28PVQmzM/yc8lgP1UrXUTnL6bo7zMX9f7yPzSyW10dPnVBf8UlaVyPCJ1OuGx1N6p7ixZSQCEv3Vu0kNzdTKpW+sa3+n+eStTlq9ckScMeuU+rth1TerpZ7RuW1QtdauuJ179Xenreulwqr4/33XDl32twHJPZbNzFk25uboqNjVVQ0K3vRGE2m/XTTz/pgQeyd6vQq9f/u489bdkco769n8zU3q59R02c/Lpjg3Eg8rbm7Hnf9OmihZo/92OdPXtG4RUqauToMapWrbrRYeWYcWNHa/OmTTpzJl4FCxVS+fLh6t2nnyIa5L3nAdwNVxvvm4zOO6Ddu3f92sZVi2nVlM6Z2hes3qun314t6cbD+J56sIr8C3pp455TGjLzZx0+ecHS94NhD6jHA5UyHaPlyC/0666TkqQfojqpRpkgeXm4a9fRM3ptcYxWbT1213FL0vlvhtj0+rtl9Hg7Wm75veada77qzizmz0SHvVe9Mn7/3SkPMnSSUapUKW3dujXTXR1s5ehJBgAA9mTLJCMvM2qSAWMwybjBWScZhg7v0aNHjXx7AAAAIBMnfnyFwxi+8BsAAACAc8nFhSoAAADA8Shk2I5KBgAAAAC7opIBAAAAZEQpw2ZUMgAAAADYFZMMAAAAAHbF5VIAAABABiaul7IZlQwAAAAAdkUlAwAAAMiAh/HZjkoGAAAAALuikgEAAABkQCHDdlQyAAAAANgVlQwAAAAgI0oZNqOSAQAAAMCuqGQAAAAAGfCcDNtRyQAAAABgV1QyAAAAgAx4TobtqGQAAAAAsCsqGQAAAEAGFDJsRyUDAAAAgF2ZzGaz2egg7O3qdaMjAAAA2VXk8XlGh2CIs4t7GR2CIbxz8fU0O/6+6LD3qn5vIYe9lyNRyQAAAABgV7l4DgkAAAA4Hs/JsB2VDAAAAAB2xSQDAAAAgF1xuRQAAACQAQ/jsx2VDAAAAAB2RSUDAAAAyIBChu2oZAAAAAB5QFRUlOrUqaNChQopKChIHTp00IEDB6z6XL16VYMGDVJgYKAKFiyoRx55RHFxcVZ9jh8/rrZt26pAgQIKCgrSiBEjdP26fR80xyQDAAAAyMjkwC0b1q1bp0GDBmnTpk1avXq1rl27ppYtW+ry5cuWPsOGDdO3336rzz//XOvWrdOpU6fUqVMny/60tDS1bdtWqamp2rhxo+bPn6958+bplVdeyV4w/4EnfgMAgFyBJ367ltz8xO/dJy857L2qFCt41689c+aMgoKCtG7dOjVp0kSJiYkqWrSoFi9erM6dO0uS9u/fr4oVKyo6Olr169fXDz/8oIceekinTp1ScHCwJGn27NkaOXKkzpw5I09PT7vkRSUDAAAAyMDkwP+lpKQoKSnJaktJSclSnImJiZKkwoULS5K2bduma9euKTIy0tKnQoUKKl68uKKjoyVJ0dHRqlq1qmWCIUmtWrVSUlKS9uzZY6+PkEkGAAAAYJSoqCj5+flZbVFRUf/5uvT0dA0dOlQNGzZUlSpVJEmxsbHy9PSUv7+/Vd/g4GDFxsZa+mScYNzcf3OfveTiQhUAAADgeI58TsaoUaM0fPhwqzYvL6//fN2gQYO0e/dubdiwIadCswmTDAAAAMAgXl5eWZpUZDR48GCtWLFC69ev1z333GNpDwkJUWpqqi5cuGBVzYiLi1NISIilz+bNm62Od/PuUzf72AOXSwEAAAAZ5NKbS8lsNmvw4MFatmyZ1q5dq1KlSlntr1Wrljw8PLRmzRpL24EDB3T8+HFFRERIkiIiIrRr1y7Fx8db+qxevVq+vr6qVKlSNiO6PSoZAAAAQB4waNAgLV68WF9//bUKFSpkWUPh5+en/Pnzy8/PT3369NHw4cNVuHBh+fr66tlnn1VERITq168vSWrZsqUqVaqkHj16aOrUqYqNjdWYMWM0aNCgbFdU7oRJBgAAAJBRLn3k96xZsyRJzZo1s2qfO3euevXqJUl6++235ebmpkceeUQpKSlq1aqVZs6caenr7u6uFStWaODAgYqIiJCPj4969uypCRMm2DVWnpMBAAByBZ6T4Vpy83My9p2+/N+d7KRiqI/D3suRcvHwAgAAAI5nyq2ljDyEhd8AAAAA7IpJhh0tWbxIrR9orjo1q6p7ty7atXOn0SHlqG1bt+jZZwYoslkjVa8crrVrfjI6JIdytfG+ibxdI29X/fftqnnf5GzneUHvfJrSs672vt9ZZxY+oZ8mttF9ZQIt+y8t7XXLbcjDlS19Anw89fGzjXVq3uM6MfdxvT+ggXy88vaFIK5+nmeFyeS4zVkxybCTlT98r2lTo9T/mUFa8vkyhYdX0MD+fZSQkGB0aDkmOfmKwsPDNWrMOKNDcThXHG+JvF0pb1f99+2qeUvOeZ6/P6ChmlcLVb8Zv6re819r7c5T+nZsK4UGFJAkle73mdU2YOYGpaeb9XXMMcsxPn6uiSreG6B2k1apy+s/qWHFEL3Xv4FRKdmFK5/ncBwmGXayYP5cdercVR06PqIyZctqzLjx8vb21vKvvjQ6tBzTqHFTDR4yTC0iHzA6FIdzxfGWyNuV8nbVf9+umrfkfOe5t4e72tcroTELt+m3fXE6EndRkz/friOxSerXMlySFJ+YbLW1rVNc6/ec1l/xlyRJ4cX81LLmPRo0+zdtPXxW0Qfi9cInMercoJRCAvIbmZ5NXPk8h+MwybCDa6mp2rd3j+pH/PPNhpubm+rXb6CdO/4wMDLkBFcdb/J2rbzhWpzxPM/nblI+dzelXEuzak9OTVNEheBM/YP8vPVgzXs0f+0hS1vd8kV1/lKK/jjyTzXn512nlG42q07ZojkXPAyXWx/Gl5cwybCD8xfOKy0tTYGBgVbtgYGBOnv2rEFRIae46niTt2vlDdfijOf5pavXtelAvEY+Ul0hAfnlZjLp0calVa98UQXfogrxeNOyunj1mr7ZfNzSFuyfX2eSrlr1S0s36/ylFAX7591KBuAIhk8yZsyYoSeffFJLliyRJC1YsECVKlVShQoVNHr0aF2/fueHXqSkpCgpKclqS0lJcUToAAAgF+s341eZTNLhOY/q3OIeGti6oj7/7ajM6ZkfEfbk/eW09NcjmSofcFGUMmxm6CRj0qRJGj16tK5cuaJhw4ZpypQpGjZsmLp3766ePXvqo48+0sSJE+94jKioKPn5+Vltb0yJclAGNwT4B8jd3T3T4riEhAQVKVLEobEg57nqeJO3a+UN1+Ks5/nRuIt68NWVCuqxUOEDP1ez0d/Jw91NR+MvWvVrUCFI5Yv5ad7ag1btcReSVdTX26rN3c2kgIJeiruQnOPxA3mZoZOMefPmad68efriiy+0cuVKvfzyy3r33Xf18ssva9SoUZozZ44WL158x2OMGjVKiYmJVtuIkaMclMENHp6eqlipsmI2RVva0tPTFRMTrWrVazo0FuQ8Vx1v8natvOFanP08v5JyXXEXkuXv46kW1Yvpuy1/W+1/snl5/f7nWe0+dt6qffPBMwoo6KUapf65jKxplVC5mUzacviMQ2KHMUwO/J+zMvRGz6dOnVLt2rUlSdWrV5ebm5tq1Khh2X/ffffp1KlTdzyGl5eXvLy8rNqu3vkKqxzRo2dvjR09UpUrV1GVqtW0cMF8JScnq0PHTo4PxkGuXL6s48f/uXb15IkT2r9vn/z8/BQaFmZgZDnPFcdbIm9XyttV/327at6Sc57nLaqHySSTDp1KVOmQQnqtRx0dPJmoBb/8s7i7UH4PdaxfQqMXbM30+gMnE7XqjxOa0b+BhnwYLY98bnrzqXr6YuNRxZ7Pu5UMVz7P4TiGTjJCQkK0d+9eFS9eXIcOHVJaWpr27t2rypVvPARnz549CgoKMjLELHuwdRudP3dOM2dM19mzZxReoaJmzvlIgXm4zPxf9uzZrb69n7T8PG3qjcvU2rXvqImTXzcqLIdwxfGWyNuV8nbVf9+umrfknOe5XwFPvfrYfSoW6KPzl1L0dcwxjf/0d11P+2dNRucGpWQymfT5hiO3PEaf6ev1Zp/6WvFKK6WbbzxDY8QnMY5KIUe48nmeVc78kDxHMZnN5syrnxxk7NixmjNnjtq3b681a9bo0Ucf1eLFizVq1CiZTCa99tpr6ty5s956661sHdeISgYAALBNkcfnGR2CIc4u7mV0CIbwzsUPTj8c77hKVdkg57xTmaHDO378eOXPn1/R0dHq16+fXnrpJVWvXl0vvviirly5oocffvg/F34DAAAA9kQhw3aGVjJyCpUMAADyHioZriU3VzL+dGAlowyVDAAAAMAFUMqwmeEP4wMAAADgXKhkAAAAABk48/MrHIVKBgAAAAC7opIBAAAAZMBzMmxHJQMAAACAXVHJAAAAADKgkGE7KhkAAAAA7IpKBgAAAJARpQybUckAAAAAYFdMMgAAAADYFZdLAQAAABnwMD7bUckAAAAAYFdUMgAAAIAMeBif7ahkAAAAALArKhkAAABABhQybEclAwAAAIBdUckAAAAAMmBNhu2oZAAAAACwKyoZAAAAgBVKGbYymc1ms9FB2NvV60ZHACCnON9vrKyhdA84r4BOs4wOwRDJ3ww0OoTbOnE+1WHvdU+Ap8Pey5GoZAAAAAAZ8MWO7ViTAQAAAMCuqGQAAAAAGVDIsB2VDAAAAAB2RSUDAAAAyIA1GbajkgEAAADArqhkAAAAABmYWJVhMyoZAAAAAOyKSQYAAAAAu+JyKQAAACAjrpayGZUMAAAAAHZFJQMAAADIgEKG7ahkAAAAALArKhkAAABABjyMz3ZUMgAAAADYFZUMAAAAIAMexmc7KhkAAAAA7IpKBgAAAJARhQybUckAAAAAYFdUMgAAAIAMKGTYjkqGHS1ZvEitH2iuOjWrqnu3Ltq1c6fRITkEeZO3s4uLi9PokS+oacN6qlermjp3fFh7du8yOiyHcMXxlsibvPNO3g0rh+qLMa11ZO6TSv5moB6uVzJTn7GP19GReU/q3Of99N2Eh1Um1M9qf0BBL80d3kJxS/ro9OKnNOvZZvLx/ud76HLF/LVyUjv99b+eOv9FP+39oLvGda+rfO78GYnb4+ywk5U/fK9pU6PU/5lBWvL5MoWHV9DA/n2UkJBgdGg5irzJ29nzTkpMVK8ejymfh4dmzP5QX339nYa/MFK+vn7//eI8zhXHWyJv8s5beft4eWjX0QQNnfPrLfc/36mGnnmoqp6btV5NRnypyynX9O34h+Tl4W7pM/f5SFUsXlgPvfKtHpn4vRpVDtX7g5pZ9l+7nqZFPx/Qw6+sUPWBn2rER7+pd6uKGvt4nZxOzzAmk+M2Z8Ukw04WzJ+rTp27qkPHR1SmbFmNGTde3t7eWv7Vl0aHlqPIm7ydPe+5n3yokJAQTZgUpapVq6nYPfeqQcNGurd4caNDy3GuON4SeZN33sp71e/HNX7RZn2z6egt9w9qV01Tlm7Tipi/tPuvc+r79lqFFi6gdvVLSZLC7/FXq1rF9cyMX7TlYLw27ovV8A82qEvjsgotXECS9FfcRS1Yc0C7/krQ8TOX9N3mv/TZL4fUsFKow/JE3sMkww6upaZq3949qh/RwNLm5uam+vUbaOeOPwyMLGeRN3m7Qt7rfl6rSpWr6IXhz+n+JhF6tHMHffnFUqPDynGuOt7kTd7OlHfJ4EIKLeyjtTtOWNqSrqRqy8F41QsPliTVqxCi85dS9PvhM5Y+a7efULrZrDrlg2953NKhvnrgvnv16+5TOZuAgUwO/J+zMnSScfr0ab3yyitq3ry5KlasqMqVK+vhhx/Wxx9/rLS0NCNDy5bzF84rLS1NgYGBVu2BgYE6e/asQVHlPPImb8n58z5x4m99/tmnKl68pGbN+VhdHn1MU6Mm6ZuvlxkdWo5y1fEmb/KWnCfvkIAblYj4C8lW7fEXrij4//cFBxTQmX/tT0s369zFFEufm36e0lHnv+inPXO667e9pzVh8eYcjB55nWGTjK1bt6pixYr6/vvvde3aNR06dEi1atWSj4+PXnjhBTVp0kQXL178z+OkpKQoKSnJaktJSXFABgBcQXq6WRUqVtZzQ4erQsVK6tzlUXV6pKu+WLrE6NAAwKF6vLFKEcO+UM9pq9W6dgkN61jD6JByDGsybGfYJGPo0KEaNmyYtm7dql9//VXz5s3TwYMHtWTJEh05ckRXrlzRmDFj/vM4UVFR8vPzs9remBLlgAz+EeAfIHd390yLxBISElSkSBGHxuJI5E3ekvPnXbRoUZUpU8aqrVTp0jp92nkvE5Bcd7zJm7wl58k79vwVSVKQf36r9iD/Aor7/31x56+o6L/2u7uZVLiQl6XPTSfOXtb+v89r6frDGvO/TXr5sdpyc3Piv5JhE8MmGb///rt69Ohh+fnxxx/X77//rri4OAUEBGjq1Kn64osv/vM4o0aNUmJiotU2YuSonAw9Ew9PT1WsVFkxm6Itbenp6YqJiVa16jUdGosjkTd5u0Le1Wvep7/+sl5QeezYXwoNLWZQRI7hquNN3uTtTHn/FXdRp89d1v3V77G0FcrvoTrlgxRzIE6SFLM/VgEFvVSzzD+TqmbVisnNZNKWg3G3PbabySQPdze5OfNX8bCJYQ/jCwoK0unTp1W6dGlJN+5Df/36dfn6+kqSypUrp3Pnzv3ncby8vOTl5WXVdvW6/eP9Lz169tbY0SNVuXIVValaTQsXzFdycrI6dOzk+GAciLzJ29nzfqJHT/Xq8Zg++mC2Wj7YWrt37dSXXyzV2HETjA4tx7nieEvkTd55K28f73xWz70oGeyraqUCdf5iiv4+e0nvf7NTI7vW0uFTiforLknjutfV6XNXLHejOnDign7cdlzvD26m52aul0c+N73dv7E+//WwTp+7Ucno1rScrl1P1+5jCUq5lqZaZYM08cl6+mLDn7qelm5I3sj9DJtkdOjQQQMGDNAbb7whLy8vTZw4UU2bNlX+/DdKdgcOHFCxYnnnm8IHW7fR+XPnNHPGdJ09e0bhFSpq5pyPFOgE5dY7IW/ydva8q1StprfemaHp776lD2a/r2LF7tGIkaPV9qF2RoeW41xxvCXyJu+8lfd9ZYO0anJ7y89T+zaUJC1Ys19Pv/uz3vxquwp4e2jGoKby9/HUxr2xavfqCqVc++cGO73f/Elv92+s7yc+rHSzWcujj+j5DzZY9l9PS9fwR2qqXJifTCaTjp+5qFnf7dZ7X+edhxbC8Uxms9lsxBtfunRJffr00VdffaW0tDRFRERo4cKFKlXqxn2bV61apcTERHXp0iXbxzaikgHAMYz5jWU8rkgAnFdAp1lGh2CI5G8GGh3CbV1IdtxdTv3zu/93pzzIsEnGTVevXtX169dVsGBB+x2TSQbgtJhkAHA2TDJyHyYZtjPscqmbvL29jQ4BAAAAsHDmh+Q5Ck/8BgAAAGBXhlcyAAAAgNyES1RtRyUDAAAAgF1RyQAAAAAyoJBhOyoZAAAAAOyKSgYAAACQEaUMm1HJAAAAAGBXVDIAAACADHhOhu2oZAAAAACwKyoZAAAAQAY8J8N2VDIAAAAA2BWVDAAAACADChm2o5IBAAAAwK6oZAAAAAAZUcqwGZUMAAAAAHbFJAMAAACAXTHJAAAAADIwOfB/d+P9999XyZIl5e3trXr16mnz5s12/gRsxyQDAAAAyCM+++wzDR8+XOPGjdPvv/+u6tWrq1WrVoqPjzc6NCtMMgAAAIAMTCbHbdn11ltvqV+/furdu7cqVaqk2bNnq0CBAvrkk0/s/0HYgEkGAAAAYJCUlBQlJSVZbSkpKbfsm5qaqm3btikyMtLS5ubmpsjISEVHRzsq5Kwxw26uXr1qHjdunPnq1atGh+JQ5E3eroC8ydsVkDd5w/HGjRtnlmS1jRs37pZ9T548aZZk3rhxo1X7iBEjzHXr1nVAtFlnMpvNZkNnOU4kKSlJfn5+SkxMlK+vr9HhOAx5k7crIG/ydgXkTd5wvJSUlEyVCy8vL3l5eWXqe+rUKRUrVkwbN25URESEpf3FF1/UunXrFBMTk+PxZhUP4wMAAAAMcrsJxa0UKVJE7u7uiouLs2qPi4tTSEhIToR311iTAQAAAOQBnp6eqlWrltasWWNpS09P15o1a6wqG7kBlQwAAAAgjxg+fLh69uyp2rVrq27dunrnnXd0+fJl9e7d2+jQrDDJsCMvLy+NGzcuyyUvZ0He5O0KyJu8XQF5kzdyv0cffVRnzpzRK6+8otjYWNWoUUMrV65UcHCw0aFZYeE3AAAAALtiTQYAAAAAu2KSAQAAAMCumGQAAAAAsCsmGQAAAADsikmGHb3//vsqWbKkvL29Va9ePW3evNnokHLU+vXr9fDDDyssLEwmk0nLly83OiSHiIqKUp06dVSoUCEFBQWpQ4cOOnDggNFh5bhZs2apWrVq8vX1la+vryIiIvTDDz8YHZbDvf766zKZTBo6dKjRoeSoV199VSaTyWqrUKGC0WE5xMmTJ/XEE08oMDBQ+fPnV9WqVbV161ajw8pRJUuWzDTeJpNJgwYNMjq0HJWWlqaxY8eqVKlSyp8/v8qUKaOJEyfKFe6Jc/HiRQ0dOlQlSpRQ/vz51aBBA23ZssXosOBEmGTYyWeffabhw4dr3Lhx+v3331W9enW1atVK8fHxRoeWYy5fvqzq1avr/fffNzoUh1q3bp0GDRqkTZs2afXq1bp27Zpatmypy5cvGx1ajrrnnnv0+uuva9u2bdq6dauaN2+u9u3ba8+ePUaH5jBbtmzRnDlzVK1aNaNDcYjKlSvr9OnTlm3Dhg1Gh5Tjzp8/r4YNG8rDw0M//PCD9u7dqzfffFMBAQFGh5ajtmzZYjXWq1evliR16dLF4Mhy1pQpUzRr1izNmDFD+/bt05QpUzR16lS99957RoeW4/r27avVq1drwYIF2rVrl1q2bKnIyEidPHnS6NDgLMywi7p165oHDRpk+TktLc0cFhZmjoqKMjAqx5FkXrZsmdFhGCI+Pt4sybxu3TqjQ3G4gIAA80cffWR0GA5x8eJFc7ly5cyrV682N23a1DxkyBCjQ8pR48aNM1evXt3oMBxu5MiR5kaNGhkdhuGGDBliLlOmjDk9Pd3oUHJU27ZtzU899ZRVW6dOnczdu3c3KCLHuHLlitnd3d28YsUKq/b77rvP/PLLLxsUFZwNlQw7SE1N1bZt2xQZGWlpc3NzU2RkpKKjow2MDI6QmJgoSSpcuLDBkThOWlqalixZosuXLysiIsLocBxi0KBBatu2rdW/c2d36NAhhYWFqXTp0urevbuOHz9udEg57ptvvlHt2rXVpUsXBQUFqWbNmvrwww+NDsuhUlNTtXDhQj311FMymUxGh5OjGjRooDVr1ujgwYOSpB07dmjDhg1q3bq1wZHlrOvXrystLU3e3t5W7fnz53eJiiUcgyd+28HZs2eVlpaW6UmLwcHB2r9/v0FRwRHS09M1dOhQNWzYUFWqVDE6nBy3a9cuRURE6OrVqypYsKCWLVumSpUqGR1WjluyZIl+//13l7peuV69epo3b57Cw8N1+vRpjR8/Xo0bN9bu3btVqFAho8PLMUeOHNGsWbM0fPhwjR49Wlu2bNFzzz0nT09P9ezZ0+jwHGL58uW6cOGCevXqZXQoOe6ll15SUlKSKlSoIHd3d6Wlpem1115T9+7djQ4tRxUqVEgRERGaOHGiKlasqODgYH366aeKjo5W2bJljQ4PToJJBmCDQYMGaffu3S7zzU94eLi2b9+uxMREffHFF+rZs6fWrVvn1BONv//+W0OGDNHq1aszfevnzDJ+k1utWjXVq1dPJUqU0NKlS9WnTx8DI8tZ6enpql27tiZPnixJqlmzpnbv3q3Zs2e7zCTj448/VuvWrRUWFmZ0KDlu6dKlWrRokRYvXqzKlStr+/btGjp0qMLCwpx+vBcsWKCnnnpKxYoVk7u7u+677z499thj2rZtm9GhwUkwybCDIkWKyN3dXXFxcVbtcXFxCgkJMSgq5LTBgwdrxYoVWr9+ve655x6jw3EIT09Py7dctWrV0pYtW/Tuu+9qzpw5BkeWc7Zt26b4+Hjdd999lra0tDStX79eM2bMUEpKitzd3Q2M0DH8/f1Vvnx5HT582OhQclRoaGimSXPFihX15ZdfGhSRYx07dkw//fSTvvrqK6NDcYgRI0bopZdeUrdu3SRJVatW1bFjxxQVFeX0k4wyZcpo3bp1unz5spKSkhQaGqpHH31UpUuXNjo0OAnWZNiBp6enatWqpTVr1lja0tPTtWbNGpe5Xt2VmM1mDR48WMuWLdPatWtVqlQpo0MyTHp6ulJSUowOI0e1aNFCu3bt0vbt2y1b7dq11b17d23fvt0lJhiSdOnSJf35558KDQ01OpQc1bBhw0y3pD548KBKlChhUESONXfuXAUFBalt27ZGh+IQV65ckZub9Z9C7u7uSk9PNygix/Px8VFoaKjOnz+vH3/8Ue3btzc6JDgJKhl2Mnz4cPXs2VO1a9dW3bp19c477+jy5cvq3bu30aHlmEuXLll9q3n06FFt375dhQsXVvHixQ2MLGcNGjRIixcv1tdff61ChQopNjZWkuTn56f8+fMbHF3OGTVqlFq3bq3ixYvr4sWLWrx4sX755Rf9+OOPRoeWowoVKpRpvY2Pj48CAwOdeh3OCy+8oIcfflglSpTQqVOnNG7cOLm7u+uxxx4zOrQcNWzYMDVo8H/t3VtIVO0ex/HfkM1kathBs8QxTVIrkSwK90UmWXkTloRCJ8UKSjtZWUYElah1UVReeKCy4q1QssQssANZGVhQGRFmKUkFXghhYeFx1nvx0uzt7rCzPTrva98PzMWs9azn+a91McyP51lr/Us5OTlKSEjQw4cPVVRUpKKiImeXNuBsNpuKi4uVlJQkF5ff4+/B4sWLlZ2dLavVqmnTpunJkyc6cuSIUlJSnF3agKuqqpJhGAoODlZjY6MyMjIUEhIypP+3YJA5+/FWQ0leXp5htVoNs9lszJ4926itrXV2SQPq9u3bhqSvPklJSc4ubUB965wlGcXFxc4ubUClpKQY/v7+htlsNry8vIz58+cb169fd3ZZTvE7PMI2MTHRmDBhgmE2mw1fX18jMTHRaGxsdHZZg+LKlSvG9OnTDYvFYoSEhBhFRUXOLmlQVFVVGZKMhoYGZ5cyaD5+/Ghs2bLFsFqtxogRI4zAwEBjz549Rmdnp7NLG3AlJSVGYGCgYTabDR8fHyMtLc1oa2tzdlkYQkyG8Ru81hIAAADAoOGeDAAAAAAORcgAAAAA4FCEDAAAAAAORcgAAAAA4FCEDAAAAAAORcgAAAAA4FCEDAAAAAAORcgAAAAA4FCEDAD4m0lOTtaSJUvs3+fNm6etW7cOeh3V1dUymUxqa2sb9LEBAP9shAwA+EnJyckymUwymUwym80KCgrSgQMH1NPTM6DjXrp0SVlZWT/VlmAAAPg7cHF2AQDwTxIbG6vi4mJ1dnbq2rVrSktL0/Dhw7V79+4+7bq6umQ2mx0y5pgxYxzSDwAAg4WZDADoB4vFIh8fH/n7+2vDhg2KiYlRRUWFfYlTdna2Jk6cqODgYEnS27dvlZCQIE9PT40ZM0ZxcXFqbm6299fb26tt27bJ09NTY8eO1c6dO2UYRp8x/3u5VGdnp3bt2iU/Pz9ZLBYFBQXp5MmTam5uVnR0tCRp9OjRMplMSk5OliTZbDbl5uYqICBArq6uCg8P18WLF/uMc+3aNU2ZMkWurq6Kjo7uUycAAP1ByACA/4Orq6u6urokSbdu3VJDQ4Nu3LihyspKdXd3a9GiRfLw8NC9e/d0//59ubu7KzY21n7M4cOHdfr0aZ06dUo1NTV6//69Ll++/MMxV69erQsXLuj48eOqr69XYWGh3N3d5efnp7KyMklSQ0ODWlpadOzYMUlSbm6uzp49q4KCAj1//lzp6elauXKl7ty5I+mvMBQfH6/Fixerrq5Oa9euVWZm5kBdNgDAEMdyKQD4BYZh6NatW6qqqtKmTZvU2toqNzc3nThxwr5M6o8//pDNZtOJEydkMpkkScXFxfL09FR1dbUWLlyoo0ePavfu3YqPj5ckFRQUqKqq6rvjvnz5UqWlpbpx44ZiYmIkSYGBgfb9X5ZWeXt7y9PTU9JfMx85OTm6efOmIiMj7cfU1NSosLBQUVFRys/P1+TJk3X48GFJUnBwsJ49e6ZDhw458KoBAH4XhAwA6IfKykq5u7uru7tbNptNy5cv1759+5SWlqawsLA+92E8ffpUjY2N8vDw6NNHR0eHmpqa9OHDB7W0tGjOnDn2fS4uLpo1a9ZXS6a+qKur07BhwxQVFfXTNTc2Nurz589asGBBn+1dXV2aMWOGJKm+vr5PHZLsgQQAgP4iZABAP0RHRys/P19ms1kTJ06Ui8u/f0bd3Nz6tG1vb9fMmTN17ty5r/rx8vL6pfFdXV37fUx7e7sk6erVq/L19e2zz2Kx/FIdAAD8CCEDAPrBzc1NQUFBP9U2IiJCJSUl8vb21qhRo77ZZsKECXrw4IHmzp0rSerp6dGjR48UERHxzfZhYWGy2Wy6c+eOfbnUf/oyk9Lb22vfNnXqVFksFr158+a7MyChoaGqqKjos622tvZ/nyQAAN/Ajd8AMEBWrFihcePGKS4uTvfu3dPr169VXV2tzZs36927d5KkLVu26ODBgyovL9eLFy+Umpr6w3dcTJo0SUlJSUpJSVF5ebm9z9LSUkmSv7+/TCaTKisr1draqvb2dnl4eGjHjh1KT0/XmTNn1NTUpMePHysvL09nzpyRJK1fv16vXr1SRkaGGhoadP78eZ0+fXqgLxEAYIgiZADAABk5cqTu3r0rq9Wq+Ph4hYaGas2aNero6LDPbGzfvl2rVq1SUlKSIiMj5eHhoaVLl/6w3/z8fC1btkypqakKCQnRunXr9OnTJ0mSr6+v9u/fr8zMTI0fP14bN26UJGVlZWnv3r3Kzc1VaGioYmNjdfXqVQUEBEiSrFarysrKVF5ervDwcBUUFCgnJ2cArw4AYCgzGd+7uxAAAAAAfgEzGQAAAAAcipABAAAAwKEIGQAAAAAcipABAAAAwKEIGQAAAAAcipABAAAAwKEIGQAAAAAcipABAAAAwKEIGQAAAAAcipABAAAAwKEIGQAAAAAc6k/kHEuieruGKQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       980\n",
      "           1       1.00      0.99      1.00      1135\n",
      "           2       1.00      1.00      1.00      1032\n",
      "           3       1.00      1.00      1.00      1010\n",
      "           4       0.99      1.00      0.99       982\n",
      "           5       0.99      1.00      0.99       892\n",
      "           6       0.99      0.99      0.99       958\n",
      "           7       1.00      0.99      1.00      1028\n",
      "           8       1.00      1.00      1.00       974\n",
      "           9       0.99      0.99      0.99      1009\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of base model = {accuracy}\")\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Base Model')\n",
    "plt.show()\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb1e90",
   "metadata": {},
   "source": [
    "Dividing training data into 10% labelled and 90% unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca8718fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "\n",
    "for _, subset_idx in strat_split.split(mnist_X, mnist_y):\n",
    "    labeled_X = mnist_X[subset_idx]\n",
    "    y = mnist_y[subset_idx]\n",
    "    unlabeled_X = mnist_X[_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e797538",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for test, train in strat_split1.split(labeled_X, y):\n",
    "    X_train = labeled_X[train]\n",
    "    y_train = y[train]\n",
    "    X_val = labeled_X[test]\n",
    "    y_val = y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c58bb87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_dataset = MNISTDataset(labeled_X, y)\n",
    "train_loader = DataLoader(labeled_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = MNISTDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "unlabeled_dataset = MNISTDataset(unlabeled_X, np.zeros(len(unlabeled_X)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f87be90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model1.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "\n",
    "model1.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12cd4eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5678/2126796601.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img).unsqueeze(0)\n",
      "/tmp/ipykernel_5678/2126796601.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.y[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.2635, Val Loss: 0.3937, Val Accuracy: 0.8819\n",
      "Epoch 2/1000\n",
      "Epoch 2/1000, Train Loss: 0.0514, Val Loss: 0.1478, Val Accuracy: 0.9557\n",
      "Epoch 3/1000\n",
      "Epoch 3/1000, Train Loss: 0.0346, Val Loss: 0.1113, Val Accuracy: 0.9667\n",
      "Epoch 4/1000\n",
      "Epoch 4/1000, Train Loss: 0.0218, Val Loss: 0.1708, Val Accuracy: 0.9475\n",
      "Epoch 5/1000\n",
      "Epoch 5/1000, Train Loss: 0.0163, Val Loss: 0.1322, Val Accuracy: 0.9613\n",
      "Epoch 6/1000\n",
      "Epoch 6/1000, Train Loss: 0.0178, Val Loss: 0.1096, Val Accuracy: 0.9669\n",
      "Epoch 7/1000\n",
      "Epoch 7/1000, Train Loss: 0.0133, Val Loss: 0.0885, Val Accuracy: 0.9734\n",
      "Epoch 8/1000\n",
      "Epoch 8/1000, Train Loss: 0.0135, Val Loss: 0.0707, Val Accuracy: 0.9827\n",
      "Epoch 9/1000\n",
      "Epoch 9/1000, Train Loss: 0.0193, Val Loss: 0.1289, Val Accuracy: 0.9735\n",
      "Epoch 10/1000\n",
      "Epoch 10/1000, Train Loss: 0.0200, Val Loss: 0.0543, Val Accuracy: 0.9842\n",
      "Epoch 11/1000\n",
      "Epoch 11/1000, Train Loss: 0.0218, Val Loss: 0.2099, Val Accuracy: 0.9537\n",
      "Epoch 12/1000\n",
      "Epoch 12/1000, Train Loss: 0.0261, Val Loss: 0.1304, Val Accuracy: 0.9760\n",
      "Epoch 13/1000\n",
      "Epoch 13/1000, Train Loss: 0.0141, Val Loss: 0.0486, Val Accuracy: 0.9889\n",
      "Epoch 14/1000\n",
      "Epoch 14/1000, Train Loss: 0.0141, Val Loss: 0.0538, Val Accuracy: 0.9870\n",
      "Epoch 15/1000\n",
      "Epoch 15/1000, Train Loss: 0.0026, Val Loss: 0.0662, Val Accuracy: 0.9847\n",
      "Epoch 16/1000\n",
      "Epoch 16/1000, Train Loss: 0.0092, Val Loss: 0.3228, Val Accuracy: 0.9090\n",
      "Epoch 17/1000\n",
      "Epoch 17/1000, Train Loss: 0.0160, Val Loss: 0.0554, Val Accuracy: 0.9854\n",
      "Epoch 18/1000\n",
      "Epoch 18/1000, Train Loss: 0.0021, Val Loss: 0.0430, Val Accuracy: 0.9896\n",
      "Epoch 19/1000\n",
      "Epoch 19/1000, Train Loss: 0.0004, Val Loss: 0.0294, Val Accuracy: 0.9926\n",
      "Epoch 20/1000\n",
      "Epoch 20/1000, Train Loss: 0.0003, Val Loss: 0.0279, Val Accuracy: 0.9923\n",
      "Epoch 21/1000\n",
      "Epoch 21/1000, Train Loss: 0.0002, Val Loss: 0.0288, Val Accuracy: 0.9927\n",
      "Epoch 22/1000\n",
      "Epoch 22/1000, Train Loss: 0.0001, Val Loss: 0.0278, Val Accuracy: 0.9929\n",
      "Epoch 23/1000\n",
      "Epoch 23/1000, Train Loss: 0.0000, Val Loss: 0.0282, Val Accuracy: 0.9929\n",
      "Epoch 24/1000\n",
      "Epoch 24/1000, Train Loss: 0.0001, Val Loss: 0.0290, Val Accuracy: 0.9932\n",
      "Epoch 25/1000\n",
      "Epoch 25/1000, Train Loss: 0.0001, Val Loss: 0.0300, Val Accuracy: 0.9926\n",
      "Epoch 26/1000\n",
      "Epoch 26/1000, Train Loss: 0.0001, Val Loss: 0.0286, Val Accuracy: 0.9933\n",
      "Epoch 27/1000\n",
      "Epoch 27/1000, Train Loss: 0.0000, Val Loss: 0.0283, Val Accuracy: 0.9931\n",
      "Epoch 28/1000\n",
      "Epoch 28/1000, Train Loss: 0.0000, Val Loss: 0.0284, Val Accuracy: 0.9929\n",
      "Epoch 29/1000\n",
      "Early stopping at epoch 29\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "loss_list_10 = []\n",
    "accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model1.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model1(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model1.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model1(X_batch)\n",
    "            test_loss = loss_fn(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model1.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77581920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5678/2126796601.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img).unsqueeze(0)\n",
      "/tmp/ipykernel_5678/2126796601.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.y[idx], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "pseudo_images_07 = []\n",
    "pseudo_labels_07 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model1.load_state_dict(best_model_weights)\n",
    "    model1.eval()\n",
    "    model1.to(device)\n",
    "\n",
    "    for images, _ in unlabeled_loader:\n",
    "        images = images.to(device)  \n",
    "        outputs = model1(images)\n",
    "        preds = F.softmax(outputs, dim=1)\n",
    "\n",
    "        conf, preds = torch.max(preds, dim=1)\n",
    "        mask_07 = conf >= 0.7\n",
    "        mask_08 = conf >= 0.8\n",
    "        mask_09 = conf >= 0.9\n",
    "\n",
    "        pseudo_images_07.append(images[mask_07].cpu())\n",
    "        pseudo_labels_07.append(preds[mask_07].cpu())\n",
    "\n",
    "pseudo_images_07 = torch.cat(pseudo_images_07, dim=0)\n",
    "pseudo_labels_07 = torch.cat(pseudo_labels_07, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b72b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6261/2126796601.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img).unsqueeze(0)\n",
      "/tmp/ipykernel_6261/2126796601.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.y[idx], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pseudo_images_08 = []\n",
    "pseudo_labels_08 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model1.load_state_dict(best_model_weights)\n",
    "    model1.eval()\n",
    "    model1.to(device)\n",
    "\n",
    "    for images, _ in unlabeled_loader:\n",
    "        images = images.to(device)  \n",
    "        outputs = model1(images)\n",
    "        preds = F.softmax(outputs, dim=1)\n",
    "\n",
    "        conf, preds = torch.max(preds, dim=1)\n",
    "        mask_07 = conf >= 0.7\n",
    "        mask_08 = conf >= 0.8\n",
    "        mask_09 = conf >= 0.9\n",
    "\n",
    "\n",
    "        pseudo_images_08.append(images[mask_08].cpu())\n",
    "        pseudo_labels_08.append(preds[mask_08].cpu())\n",
    "\n",
    "\n",
    "pseudo_images_08 = torch.cat(pseudo_images_08, dim=0)\n",
    "pseudo_labels_08 = torch.cat(pseudo_labels_08, dim=0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66796938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6261/2126796601.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img).unsqueeze(0)\n",
      "/tmp/ipykernel_6261/2126796601.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.y[idx], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pseudo_images_09 = []\n",
    "pseudo_labels_09 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model1.load_state_dict(best_model_weights)\n",
    "    model1.eval()\n",
    "    model1.to(device)\n",
    "\n",
    "    for images, _ in unlabeled_loader:\n",
    "        images = images.to(device)  \n",
    "        outputs = model1(images)\n",
    "        preds = F.softmax(outputs, dim=1)\n",
    "\n",
    "        conf, preds = torch.max(preds, dim=1)\n",
    "        mask_07 = conf >= 0.7\n",
    "        mask_08 = conf >= 0.8\n",
    "        mask_09 = conf >= 0.9\n",
    "\n",
    "\n",
    "        pseudo_images_09.append(images[mask_09].cpu())\n",
    "        pseudo_labels_09.append(preds[mask_09].cpu())\n",
    "\n",
    "pseudo_images_09 = torch.cat(pseudo_images_09, dim=0)\n",
    "pseudo_labels_09 = torch.cat(pseudo_labels_09, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9da5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx].reshape(28, 28).astype(\"float32\") / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        img = img.repeat(3, 1, 1)\n",
    "\n",
    "        img = F.interpolate(img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "    \n",
    "combined_dataset_07 = ConcatDataset([PseudoDataset(pseudo_images_07, pseudo_labels_07), labeled_train_dataset])\n",
    "combined_loader_07 = DataLoader(combined_dataset_07, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_08 = ConcatDataset([PseudoDataset(pseudo_images_08, pseudo_labels_08), labeled_train_dataset])\n",
    "combined_loader_08 = DataLoader(combined_dataset_08, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_09 = ConcatDataset([PseudoDataset(pseudo_images_09, pseudo_labels_09), labeled_train_dataset])\n",
    "combined_loader_09 = DataLoader(combined_dataset_09, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_07 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model1_07.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model1_07.to(device)\n",
    "loss_fn_07 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_07 = torch.optim.Adam(model1_07.parameters(), lr=0.001)\n",
    "\n",
    "model1_08 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model1_08.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model1_08.to(device)\n",
    "loss_fn_08 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_08 = torch.optim.Adam(model1_08.parameters(), lr=0.001)\n",
    "\n",
    "model1_09 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model1_09.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model1_09.to(device)\n",
    "loss_fn_09 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_09 = torch.optim.Adam(model1_09.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c79ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model1_07.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_07:\n",
    "        optimizer_07.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model1_07(X_batch)\n",
    "        loss = loss_fn_07(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_07.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model1_07.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model1_07(X_batch)\n",
    "            test_loss = loss_fn_07(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model1_07.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model1_07.load_state_dict(best_model_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702094c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model1_08.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_08:\n",
    "        optimizer_08.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model1_08(X_batch)\n",
    "        loss = loss_fn_08(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_08.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model1_08.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model1_08(X_batch)\n",
    "            test_loss = loss_fn_08(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model1_08.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model1_08.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb52636",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model1_09.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_09:\n",
    "        optimizer_09.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model1_09(X_batch)\n",
    "        loss = loss_fn_09(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_09.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model1_09.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model1_09(X_batch)\n",
    "            test_loss = loss_fn_09(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model1_09.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model1_09.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909dceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model1_07.eval()\n",
    "    model1_07.to(device)\n",
    "    model1_08.eval()\n",
    "    model1_08.to(device)\n",
    "    model1_09.eval()\n",
    "    model1_09.to(device)\n",
    "\n",
    "    y_pred = []\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs_07 = model1_07(X_batch)\n",
    "        outputs_08 = model1_08(X_batch)\n",
    "        outputs_09 = model1_09(X_batch)\n",
    "\n",
    "        preds_07 = torch.argmax(outputs_07, dim=1)\n",
    "        preds_08 = torch.argmax(outputs_08, dim=1)\n",
    "        preds_09 = torch.argmax(outputs_09, dim=1)\n",
    "\n",
    "\n",
    "accuracy_07 = accuracy_score(mnist_test_y, preds_07)\n",
    "conf_matrix_07 = confusion_matrix(mnist_test_y, preds_07)\n",
    "class_report_07 = classification_report(mnist_test_y, preds_07)\n",
    "\n",
    "accuracy_08 = accuracy_score(mnist_test_y, preds_08)\n",
    "conf_matrix_08 = confusion_matrix(mnist_test_y, preds_08)\n",
    "class_report_08 = classification_report(mnist_test_y, preds_08)\n",
    "\n",
    "accuracy_09 = accuracy_score(mnist_test_y, preds_09)\n",
    "conf_matrix_09 = confusion_matrix(mnist_test_y, preds_09)\n",
    "class_report_09 = classification_report(mnist_test_y, preds_09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of model trained with 10% labeled data and pseudo-labeling:\")\n",
    "print(f\"0.7 Confidence Threshold: {accuracy_07}\")\n",
    "print(f\"0.8 Confidence Threshold: {accuracy_08}\")\n",
    "print(f\"0.9 Confidence Threshold: {accuracy_09}\")\n",
    "\n",
    "plt.figure(figsize=(30, 8))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    if i == 0:\n",
    "        sns.heatmap(conf_matrix_07, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.7 Threshold)')\n",
    "    elif i == 1:\n",
    "        sns.heatmap(conf_matrix_08, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.8 Threshold)')\n",
    "    else:\n",
    "        sns.heatmap(conf_matrix_09, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.9 Threshold)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report (0.7 Threshold):\")\n",
    "print(class_report_07)\n",
    "print(\"\\nClassification Report (0.8 Threshold):\")\n",
    "print(class_report_08)\n",
    "print(\"\\nClassification Report (0.9 Threshold):\")\n",
    "print(class_report_09)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8169a",
   "metadata": {},
   "source": [
    "Training a model on 20% labeled and 80% unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160882fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for _, subset_idx in strat_split.split(mnist_X, mnist_y):\n",
    "    labeled_X = mnist_X[subset_idx]\n",
    "    y = mnist_y[subset_idx]\n",
    "    unlabeled_X = mnist_X[_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b16fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for test, train in strat_split1.split(labeled_X, y):\n",
    "    X_train = labeled_X[train]\n",
    "    y_train = y[train]\n",
    "    X_val = labeled_X[test]\n",
    "    y_val = y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_dataset = MNISTDataset(labeled_X, y)\n",
    "train_loader = DataLoader(labeled_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = MNISTDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "unlabeled_dataset = MNISTDataset(unlabeled_X, np.zeros(len(unlabeled_X)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model2.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "\n",
    "model2.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ac5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "loss_list_10 = []\n",
    "accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model2.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model2(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model2.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model2(X_batch)\n",
    "            test_loss = loss_fn(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model2.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc454bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_images_07 = []\n",
    "pseudo_labels_07 = []\n",
    "pseudo_images_08 = []\n",
    "pseudo_labels_08 = []\n",
    "pseudo_images_09 = []\n",
    "pseudo_labels_09 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model2.load_state_dict(best_model_weights)\n",
    "    model2.eval()\n",
    "    model2.to(device)\n",
    "\n",
    "    for images, _ in unlabeled_X:\n",
    "        images = images.to(device)\n",
    "        outputs = model2(images)\n",
    "        preds = F.softmax(outputs, dim=1)\n",
    "\n",
    "        conf, preds = torch.max(preds, dim=1)\n",
    "        mask_07 = conf >= 0.7\n",
    "        mask_08 = conf >= 0.8\n",
    "        mask_09 = conf >= 0.9\n",
    "\n",
    "        pseudo_images_07.append(images[mask_07].cpu())\n",
    "        pseudo_labels_07.append(preds[mask_07].cpu())\n",
    "        pseudo_images_08.append(images[mask_08].cpu())\n",
    "        pseudo_labels_08.append(preds[mask_08].cpu())\n",
    "        pseudo_images_09.append(images[mask_09].cpu())\n",
    "        pseudo_labels_09.append(preds[mask_09].cpu())\n",
    "\n",
    "pseudo_images_07 = torch.cat(pseudo_images_07, dim=0)\n",
    "pseudo_labels_07 = torch.cat(pseudo_labels_07, dim=0)\n",
    "pseudo_images_08 = torch.cat(pseudo_images_08, dim=0)\n",
    "pseudo_labels_08 = torch.cat(pseudo_labels_08, dim=0)\n",
    "pseudo_images_09 = torch.cat(pseudo_images_09, dim=0)\n",
    "pseudo_labels_09 = torch.cat(pseudo_labels_09, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx].reshape(28, 28).astype(\"float32\") / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        img = img.repeat(3, 1, 1)\n",
    "\n",
    "        img = F.interpolate(img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "    \n",
    "combined_dataset_07 = ConcatDataset([PseudoDataset(pseudo_images_07, pseudo_labels_07), labeled_train_dataset])\n",
    "combined_loader_07 = DataLoader(combined_dataset_07, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_08 = ConcatDataset([PseudoDataset(pseudo_images_08, pseudo_labels_08), labeled_train_dataset])\n",
    "combined_loader_08 = DataLoader(combined_dataset_08, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_09 = ConcatDataset([PseudoDataset(pseudo_images_09, pseudo_labels_09), labeled_train_dataset])\n",
    "combined_loader_09 = DataLoader(combined_dataset_09, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c654cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_07 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model2_07.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model2_07.to(device)\n",
    "loss_fn_07 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_07 = torch.optim.Adam(model2_07.parameters(), lr=0.001)\n",
    "\n",
    "model2_08 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model2_08.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model2_08.to(device)\n",
    "loss_fn_08 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_08 = torch.optim.Adam(model2_08.parameters(), lr=0.001)\n",
    "\n",
    "model2_09 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model2_09.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model2_09.to(device)\n",
    "loss_fn_09 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_09 = torch.optim.Adam(model2_09.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a89f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model2_07.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_07:\n",
    "        optimizer_07.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model2_07(X_batch)\n",
    "        loss = loss_fn_07(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_07.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model2_07.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model2_07(X_batch)\n",
    "            test_loss = loss_fn_07(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model2_07.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model2_07.load_state_dict(best_model_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model2_08.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_08:\n",
    "        optimizer_08.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model2_08(X_batch)\n",
    "        loss = loss_fn_08(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_08.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model2_08.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model2_08(X_batch)\n",
    "            test_loss = loss_fn_08(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model2_08.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model2_08.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model2_09.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_09:\n",
    "        optimizer_09.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model2_09(X_batch)\n",
    "        loss = loss_fn_09(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_09.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model2_09.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model2_09(X_batch)\n",
    "            test_loss = loss_fn_09(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model2_09.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model2_09.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc9e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model2_07.eval()\n",
    "    model2_07.to(device)\n",
    "    model2_08.eval()\n",
    "    model2_08.to(device)\n",
    "    model2_09.eval()\n",
    "    model2_09.to(device)\n",
    "\n",
    "    y_pred = []\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs_07 = model2_07(X_batch)\n",
    "        outputs_08 = model2_08(X_batch)\n",
    "        outputs_09 = model2_09(X_batch)\n",
    "\n",
    "        preds_07 = torch.argmax(outputs_07, dim=1)\n",
    "        preds_08 = torch.argmax(outputs_08, dim=1)\n",
    "        preds_09 = torch.argmax(outputs_09, dim=1)\n",
    "\n",
    "\n",
    "accuracy_07 = accuracy_score(mnist_test_y, preds_07)\n",
    "conf_matrix_07 = confusion_matrix(mnist_test_y, preds_07)\n",
    "class_report_07 = classification_report(mnist_test_y, preds_07)\n",
    "\n",
    "accuracy_08 = accuracy_score(mnist_test_y, preds_08)\n",
    "conf_matrix_08 = confusion_matrix(mnist_test_y, preds_08)\n",
    "class_report_08 = classification_report(mnist_test_y, preds_08)\n",
    "\n",
    "accuracy_09 = accuracy_score(mnist_test_y, preds_09)\n",
    "conf_matrix_09 = confusion_matrix(mnist_test_y, preds_09)\n",
    "class_report_09 = classification_report(mnist_test_y, preds_09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cabaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of model trained with 20% labeled data and pseudo-labeling:\")\n",
    "print(f\"0.7 Confidence Threshold: {accuracy_07}\")\n",
    "print(f\"0.8 Confidence Threshold: {accuracy_08}\")\n",
    "print(f\"0.9 Confidence Threshold: {accuracy_09}\")\n",
    "\n",
    "plt.figure(figsize=(30, 8))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    if i == 0:\n",
    "        sns.heatmap(conf_matrix_07, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.7 Threshold)')\n",
    "    elif i == 1:\n",
    "        sns.heatmap(conf_matrix_08, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.8 Threshold)')\n",
    "    else:\n",
    "        sns.heatmap(conf_matrix_09, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.9 Threshold)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report (0.7 Threshold):\")\n",
    "print(class_report_07)\n",
    "print(\"\\nClassification Report (0.8 Threshold):\")\n",
    "print(class_report_08)\n",
    "print(\"\\nClassification Report (0.9 Threshold):\")\n",
    "print(class_report_09)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4813d27",
   "metadata": {},
   "source": [
    "Training a model on 30% labeled and 70% unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for _, subset_idx in strat_split.split(mnist_X, mnist_y):\n",
    "    labeled_X = mnist_X[subset_idx]\n",
    "    y = mnist_y[subset_idx]\n",
    "    unlabeled_X = mnist_X[_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc66cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for test, train in strat_split1.split(labeled_X, y):\n",
    "    X_train = labeled_X[train]\n",
    "    y_train = y[train]\n",
    "    X_val = labeled_X[test]\n",
    "    y_val = y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_dataset = MNISTDataset(labeled_X, y)\n",
    "train_loader = DataLoader(labeled_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = MNISTDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "unlabeled_dataset = MNISTDataset(unlabeled_X, np.zeros(len(unlabeled_X)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cdf84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model3.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "\n",
    "model3.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "loss_list_10 = []\n",
    "accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model3.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model3(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model3.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model3(X_batch)\n",
    "            test_loss = loss_fn(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model3.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_images_07 = []\n",
    "pseudo_labels_07 = []\n",
    "pseudo_images_08 = []\n",
    "pseudo_labels_08 = []\n",
    "pseudo_images_09 = []\n",
    "pseudo_labels_09 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model3.load_state_dict(best_model_weights)\n",
    "    model3.eval()\n",
    "    model3.to(device)\n",
    "\n",
    "    for images, _ in unlabeled_X:\n",
    "        images = images.to(device)\n",
    "        outputs = model3(images)\n",
    "        preds = F.softmax(outputs, dim=1)\n",
    "\n",
    "        conf, preds = torch.max(preds, dim=1)\n",
    "        mask_07 = conf >= 0.7\n",
    "        mask_08 = conf >= 0.8\n",
    "        mask_09 = conf >= 0.9\n",
    "\n",
    "        pseudo_images_07.append(images[mask_07].cpu())\n",
    "        pseudo_labels_07.append(preds[mask_07].cpu())\n",
    "        pseudo_images_08.append(images[mask_08].cpu())\n",
    "        pseudo_labels_08.append(preds[mask_08].cpu())\n",
    "        pseudo_images_09.append(images[mask_09].cpu())\n",
    "        pseudo_labels_09.append(preds[mask_09].cpu())\n",
    "\n",
    "pseudo_images_07 = torch.cat(pseudo_images_07, dim=0)\n",
    "pseudo_labels_07 = torch.cat(pseudo_labels_07, dim=0)\n",
    "pseudo_images_08 = torch.cat(pseudo_images_08, dim=0)\n",
    "pseudo_labels_08 = torch.cat(pseudo_labels_08, dim=0)\n",
    "pseudo_images_09 = torch.cat(pseudo_images_09, dim=0)\n",
    "pseudo_labels_09 = torch.cat(pseudo_labels_09, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx].reshape(28, 28).astype(\"float32\") / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        img = img.repeat(3, 1, 1)\n",
    "\n",
    "        img = F.interpolate(img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "    \n",
    "combined_dataset_07 = ConcatDataset([PseudoDataset(pseudo_images_07, pseudo_labels_07), labeled_train_dataset])\n",
    "combined_loader_07 = DataLoader(combined_dataset_07, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_08 = ConcatDataset([PseudoDataset(pseudo_images_08, pseudo_labels_08), labeled_train_dataset])\n",
    "combined_loader_08 = DataLoader(combined_dataset_08, batch_size=64, shuffle=True)\n",
    "\n",
    "combined_dataset_09 = ConcatDataset([PseudoDataset(pseudo_images_09, pseudo_labels_09), labeled_train_dataset])\n",
    "combined_loader_09 = DataLoader(combined_dataset_09, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf13f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_07 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model3_07.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model3_07.to(device)\n",
    "loss_fn_07 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_07 = torch.optim.Adam(model3_07.parameters(), lr=0.001)\n",
    "\n",
    "model3_08 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model3_08.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model3_08.to(device)\n",
    "loss_fn_08 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_08 = torch.optim.Adam(model3_08.parameters(), lr=0.001)\n",
    "\n",
    "model3_09 = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model3_09.classifier[3] = torch.nn.Linear(in_features=1024, out_features=10)\n",
    "model3_09.to(device)\n",
    "loss_fn_09 = torch.nn.CrossEntropyLoss()\n",
    "optimizer_09 = torch.optim.Adam(model3_09.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9afc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model3_07.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_07:\n",
    "        optimizer_07.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model3_07(X_batch)\n",
    "        loss = loss_fn_07(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_07.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model3_07.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model3_07(X_batch)\n",
    "            test_loss = loss_fn_07(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model3_07.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model3_07.load_state_dict(best_model_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7aa18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model3_08.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_08:\n",
    "        optimizer_08.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model3_08(X_batch)\n",
    "        loss = loss_fn_08(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_08.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model3_08.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model3_08(X_batch)\n",
    "            test_loss = loss_fn_08(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model3_08.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model3_08.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa62410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_model_loss = float('inf')\n",
    "best_model_weights = None\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "comb_loss_list_10 = []\n",
    "comb_accuracy_list_10 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model3_09.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    batch = 0\n",
    "    for X_batch, y_batch in combined_loader_09:\n",
    "        optimizer_09.zero_grad()\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model3_09(X_batch)\n",
    "        loss = loss_fn_09(outputs, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_09.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    comb_loss_list_10.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    model3_09.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    epoch_val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model3_09(X_batch)\n",
    "            test_loss = loss_fn_09(outputs, y_batch)\n",
    "            epoch_val_loss += test_loss.item()\n",
    "            \n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            accuracy = (y_pred == y_batch).float().mean()\n",
    "            epoch_val_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_loader)\n",
    "    avg_val_accuracy = epoch_val_accuracy / len(test_loader)\n",
    "    comb_accuracy_list_10.append(avg_val_accuracy)\n",
    "\n",
    "    if avg_val_loss < best_model_loss:\n",
    "        best_model_loss = avg_val_loss\n",
    "        best_model_weights = copy.deepcopy(model3_09.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "model3_09.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15356c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model3_07.eval()\n",
    "    model3_07.to(device)\n",
    "    model3_08.eval()\n",
    "    model3_08.to(device)\n",
    "    model3_09.eval()\n",
    "    model3_09.to(device)\n",
    "\n",
    "    y_pred = []\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs_07 = model3_07(X_batch)\n",
    "        outputs_08 = model3_08(X_batch)\n",
    "        outputs_09 = model3_09(X_batch)\n",
    "\n",
    "        preds_07 = torch.argmax(outputs_07, dim=1)\n",
    "        preds_08 = torch.argmax(outputs_08, dim=1)\n",
    "        preds_09 = torch.argmax(outputs_09, dim=1)\n",
    "\n",
    "\n",
    "accuracy_07 = accuracy_score(mnist_test_y, preds_07)\n",
    "conf_matrix_07 = confusion_matrix(mnist_test_y, preds_07)\n",
    "class_report_07 = classification_report(mnist_test_y, preds_07)\n",
    "\n",
    "accuracy_08 = accuracy_score(mnist_test_y, preds_08)\n",
    "conf_matrix_08 = confusion_matrix(mnist_test_y, preds_08)\n",
    "class_report_08 = classification_report(mnist_test_y, preds_08)\n",
    "\n",
    "accuracy_09 = accuracy_score(mnist_test_y, preds_09)\n",
    "conf_matrix_09 = confusion_matrix(mnist_test_y, preds_09)\n",
    "class_report_09 = classification_report(mnist_test_y, preds_09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2817e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of model trained with 30% labeled data and pseudo-labeling:\")\n",
    "print(f\"0.7 Confidence Threshold: {accuracy_07}\")\n",
    "print(f\"0.8 Confidence Threshold: {accuracy_08}\")\n",
    "print(f\"0.9 Confidence Threshold: {accuracy_09}\")\n",
    "\n",
    "plt.figure(figsize=(30, 8))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    if i == 0:\n",
    "        sns.heatmap(conf_matrix_07, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.7 Threshold)')\n",
    "    elif i == 1:\n",
    "        sns.heatmap(conf_matrix_08, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.8 Threshold)')\n",
    "    else:\n",
    "        sns.heatmap(conf_matrix_09, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix (0.9 Threshold)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report (0.7 Threshold):\")\n",
    "print(class_report_07)\n",
    "print(\"\\nClassification Report (0.8 Threshold):\")\n",
    "print(class_report_08)\n",
    "print(\"\\nClassification Report (0.9 Threshold):\")\n",
    "print(class_report_09)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
